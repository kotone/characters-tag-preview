"""
è§’è‰²å¡ç‰‡æ•°æ®ç”Ÿæˆä¸»ç¨‹åº - æ¨¡å—åŒ–ç‰ˆæœ¬

å¼‚æ­¥æ‰¹é‡å¤„ç†åŠ¨æ¼«è§’è‰²æ•°æ®ï¼šLLMç¿»è¯‘ + å›¾ç‰‡æœç´¢
"""

import asyncio
import argparse
import os
import sys
import aiohttp
from dotenv import load_dotenv
from tqdm.asyncio import tqdm

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from card_generator.config import Config
from card_generator.stats import Stats
from card_generator.file_utils import save_data, load_history_data
from card_generator.llm import load_source_name_mapping
from card_generator.data_processor import (
    load_tags_from_file,
    fetch_tags_from_url,
    apply_debug_filter,
    pipeline_batch
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–è„šæœ¬åŸºç¡€ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))


def parse_args(config: Config):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¼‚æ­¥æ‰¹é‡å¤„ç†åŠ¨æ¼«è§’è‰²æ•°æ®ï¼šLLMç¿»è¯‘ + å›¾ç‰‡æœç´¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  # åªå¤„ç†å‰10000ä¸ªè§’è‰²
  python %(prog)s --limit 10000
  
  # ç”Ÿäº§æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ•°æ®
  python %(prog)s
  
  # è‡ªå®šä¹‰å¹¶å‘æ•°
  python %(prog)s --llm-concurrency 10 --img-concurrency 20
        ''')
    
    # æ•°æ®å¤„ç†é€‰é¡¹
    parser.add_argument('--limit', type=int, default=0, 
                        help='é™åˆ¶å¤„ç†çš„æ•°æ®é‡ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œé»˜è®¤: 0ï¼‰ã€‚ä¾‹å¦‚ --limit 10000 åªå¤„ç†10000ä¸ªè§’è‰²')
    parser.add_argument('--random', action='store_true', 
                        help='éšæœºæŠ½å–æ•°æ®ï¼ˆé»˜è®¤ï¼šæŒ‰é¡ºåºï¼‰ã€‚éœ€é…åˆ --limit ä½¿ç”¨')
    parser.add_argument('--force-update', action='store_true',
                        help='å¼ºåˆ¶ä» URL é‡æ–°æ‹‰å–æºæ•°æ®ï¼ˆå¿½ç•¥æœ¬åœ°ç¼“å­˜ï¼‰')
    parser.add_argument('--debug', action='store_true',
                        help='Debug æ¨¡å¼ï¼šå¿½ç•¥å†å²æ•°æ®ï¼Œè¾“å‡ºåˆ° debug_output.jsonï¼Œä¸å½±å“æ­£å¼æ–‡ä»¶')
    
    # å¹¶å‘æ§åˆ¶
    parser.add_argument('--llm-concurrency', type=int, default=config.llm_concurrency,
                        help=f'LLM å¹¶å‘æ•°ï¼ˆé»˜è®¤: {config.llm_concurrency}ï¼‰')
    parser.add_argument('--img-concurrency', type=int, default=config.img_concurrency,
                        help=f'å›¾ç‰‡æœç´¢å¹¶å‘æ•°ï¼ˆé»˜è®¤: {config.img_concurrency}ï¼‰')
    
    # æ‰¹å¤„ç†é…ç½®
    parser.add_argument('--batch-size', type=int, default=config.batch_size,
                        help=f'æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤: {config.batch_size}ï¼‰')
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®å’Œç»Ÿè®¡
    config = Config(BASE_DIR)
    stats = Stats()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args(config)
    
    # æ£€æŸ¥ LLM é…ç½®
    config.check_llm_config()
    
    # åŠ è½½ä½œå“åç§°æ˜ å°„è¡¨
    source_name_mapping = load_source_name_mapping(config.mapping_file)
    
    # åˆå§‹åŒ–ä¿¡å·é‡
    sem_llm = asyncio.Semaphore(args.llm_concurrency)
    sem_img = asyncio.Semaphore(args.img_concurrency)
    
    # 1. è¯»å–è¾“å…¥æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œé™¤éå¼ºåˆ¶æ›´æ–°ï¼‰
    tags_dict = {}
    
    if not args.force_update and os.path.exists(config.cached_source_file):
        # ä¼˜å…ˆä»ç¼“å­˜è¯»å–
        print(f"ğŸ“‚ å‘ç°æœ¬åœ°ç¼“å­˜æ–‡ä»¶: {config.cached_source_file}")
        tags_dict = load_tags_from_file(config.cached_source_file)
        
        if not tags_dict:
            print("âš ï¸ ç¼“å­˜æ–‡ä»¶æ— æ•ˆï¼Œå°è¯•ä» URL è·å–æ•°æ®")
            tags_dict = await fetch_tags_from_url(config.input_url, config.cached_source_file)
    else:
        # ä» URL è·å–æ•°æ®å¹¶ç¼“å­˜
        if args.force_update:
            print("ğŸ”„ å¼ºåˆ¶æ›´æ–°æ¨¡å¼ï¼šä» URL é‡æ–°æ‹‰å–æ•°æ®")
        else:
            print("ğŸ“¥ æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ï¼Œä» URL è·å–æ•°æ®")
        tags_dict = await fetch_tags_from_url(config.input_url, config.cached_source_file)
    
    if not tags_dict:
        print("âŒ é”™è¯¯: æ— æ³•è·å–æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®")
        return
    
    print(f"ğŸš€ è¾“å…¥æ€»æ•°: {len(tags_dict)}")
    
    # åº”ç”¨æ•°é‡é™åˆ¶è¿‡æ»¤
    tags_dict = apply_debug_filter(tags_dict, args.limit, args.random)
    
    # Debug æ¨¡å¼æç¤º
    if args.debug:
        print("\n" + "="*60)
        print("ğŸ› DEBUG æ¨¡å¼å·²å¯ç”¨")
        print("="*60)
        print("âš ï¸  æ­¤æ¨¡å¼å°†ï¼š")
        print("  1. å¿½ç•¥å†å²æ•°æ®ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰æŒ‡å®šçš„è§’è‰²")
        print("  2. è¾“å‡ºåˆ° debug_output.jsonï¼ˆä¸å½±å“æ­£å¼æ–‡ä»¶ï¼‰")
        print("  3. æ¯æ¬¡è¿è¡Œæ¸…ç©ºä¸Šæ¬¡çš„ debug ç»“æœ")
        print("="*60 + "\n")
    
    print(f"âš¡ å¹¶å‘é…ç½®: LLM x {args.llm_concurrency} | Image x {args.img_concurrency}")
    print(f"ğŸ”„ é‡è¯•é…ç½®: LLM {config.llm_retry_times}æ¬¡ | Image {config.img_retry_times}æ¬¡")

    # 2. è¯»å–å†å²æ•°æ®
    complete_data, incomplete_tags, existing_tags = load_history_data(
        config.output_file, args.debug
    )

    # æ„å»ºå¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨
    data_to_process = [
        {"tag": tag, "color": info["color"], "content": info["content"]}
        for tag, info in tags_dict.items()
        if tag not in existing_tags or tag in incomplete_tags
    ]

    if not data_to_process:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†ï¼")
        return

    print(f"ğŸ”¥ æœ¬æ¬¡éœ€å¤„ç†: {len(data_to_process)} ä¸ªè§’è‰²")

    # 3. åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    timeout = aiohttp.ClientTimeout(total=90)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        
        # å°†æ‰€æœ‰å¾…å¤„ç†æ•°æ®åˆ†ç»„
        batches = [data_to_process[i : i + args.batch_size] for i in range(0, len(data_to_process), args.batch_size)]
        
        tasks = []
        for batch in batches:
            # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡çš„åç¨‹ä»»åŠ¡
            task = asyncio.create_task(
                pipeline_batch(session, batch, config, sem_llm, sem_img, stats, source_name_mapping)
            )
            tasks.append(task)
        
        # 4. å¼‚æ­¥æ‰§è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
        current_data = complete_data.copy()
        finished_batches = 0
        
        # ä½¿ç”¨è§’è‰²æ•°é‡è€Œä¸æ˜¯æ‰¹æ¬¡æ•°é‡æ¥æ˜¾ç¤ºè¿›åº¦
        total_characters = len(data_to_process)
        pbar = tqdm(total=total_characters, desc="ğŸš€ å¤„ç†ä¸­", unit="è§’è‰²")
        
        for coro in asyncio.as_completed(tasks):
            batch_result = await coro
            current_data.extend(batch_result)
            finished_batches += 1
            stats.total_processed += len(batch_result)
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆæŒ‰è§’è‰²æ•°é‡ï¼‰
            pbar.update(len(batch_result))
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºå®æ—¶æˆåŠŸç‡
            llm_total = stats.llm_success + stats.llm_fail
            img_total = stats.img_success + stats.img_fail
            postfix_dict = {}
            if llm_total > 0:
                postfix_dict['LLM'] = f"{stats.llm_success/llm_total*100:.0f}%"
            if img_total > 0:
                postfix_dict['å›¾ç‰‡'] = f"{stats.img_success/img_total*100:.0f}%"
            if postfix_dict:
                pbar.set_postfix(postfix_dict)
            
            # å®šæœŸå­˜ç›˜ï¼Œè€Œä¸æ˜¯æ¯æ‰¹æ¬¡éƒ½å­˜
            if finished_batches % config.save_interval_batches == 0:
                output_file = config.debug_output_file if args.debug else config.output_file
                save_data(current_data, output_file)
        
        pbar.close()
        
        # æœ€åå†ä¸€æ¬¡æ€§ä¿å­˜ï¼Œç¡®ä¿æ•°æ®å®Œæ•´
        output_file = config.debug_output_file if args.debug else config.output_file
        save_data(current_data, output_file)
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    stats.print_summary()
    
    if args.debug:
        print(f"\nğŸ› Debug æ¨¡å¼ï¼šæ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        print("âš ï¸  æ­£å¼æ–‡ä»¶æœªå—å½±å“")
    else:
        print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ {output_file}")


if __name__ == '__main__':
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
