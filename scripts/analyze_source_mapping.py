import json
import os
from collections import defaultdict

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, '..', 'output', 'character_data.json')

def analyze_source_mappings():
    """åˆ†æä½œå“åç§°çš„æ˜ å°„å…³ç³»ï¼Œæ‰¾å‡ºä¸ä¸€è‡´çš„æƒ…å†µ"""
    
    # è¯»å–æ•°æ®
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç»Ÿè®¡ä¸åŒçš„æ˜ å°„å…³ç³»
    # 1. source_en -> set of source_cn (ä¸€ä¸ªè‹±æ–‡åå¯¹åº”å¤šä¸ªä¸­æ–‡å)
    en_to_cn = defaultdict(set)
    # 2. source_cn -> set of source_en (ä¸€ä¸ªä¸­æ–‡åå¯¹åº”å¤šä¸ªè‹±æ–‡å)
    cn_to_en = defaultdict(set)
    # 3. (source_en, source_cn) ç»„åˆçš„è§’è‰²æ•°é‡
    pair_count = defaultdict(int)
    
    for item in data:
        source_en = item.get('source_en', '').strip()
        source_cn = item.get('source_cn', '').strip()
        
        # è·³è¿‡ç©ºå€¼
        if not source_en and not source_cn:
            continue
        
        if source_en:
            en_to_cn[source_en].add(source_cn)
        if source_cn:
            cn_to_en[source_cn].add(source_en)
        
        if source_en or source_cn:
            pair_count[(source_en, source_cn)] += 1
    
    # æ‰¾å‡ºä¸ä¸€è‡´çš„æ˜ å°„
    print("=" * 80)
    print("ğŸ“Š ä½œå“åç§°æ˜ å°„åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # 1. ä¸€ä¸ªè‹±æ–‡åå¯¹åº”å¤šä¸ªä¸­æ–‡åçš„æƒ…å†µ
    inconsistent_en = {en: cns for en, cns in en_to_cn.items() if len(cns) > 1}
    print(f"\nğŸ”´ ä¸€ä¸ªè‹±æ–‡ä½œå“åå¯¹åº”å¤šä¸ªä¸­æ–‡åçš„æƒ…å†µ: {len(inconsistent_en)} ä¸ª\n")
    
    if inconsistent_en:
        for en, cns in sorted(inconsistent_en.items(), key=lambda x: len(x[1]), reverse=True)[:20]:
            print(f"  ã€{en}ã€‘ -> {len(cns)} ä¸ªä¸­æ–‡å:")
            for cn in sorted(cns):
                count = sum(pair_count[(en, c)] for c in [cn])
                display_cn = f'"{cn}"' if cn else '(ç©º)'
                print(f"     - {display_cn} ({count} ä¸ªè§’è‰²)")
            print()
    
    # 2. ä¸€ä¸ªä¸­æ–‡åå¯¹åº”å¤šä¸ªè‹±æ–‡åçš„æƒ…å†µ
    inconsistent_cn = {cn: ens for cn, ens in cn_to_en.items() if len(ens) > 1 and cn}
    print(f"\nğŸŸ¡ ä¸€ä¸ªä¸­æ–‡ä½œå“åå¯¹åº”å¤šä¸ªè‹±æ–‡åçš„æƒ…å†µ: {len(inconsistent_cn)} ä¸ª\n")
    
    if inconsistent_cn:
        for cn, ens in sorted(inconsistent_cn.items(), key=lambda x: len(x[1]), reverse=True)[:20]:
            print(f"  ã€{cn}ã€‘ -> {len(ens)} ä¸ªè‹±æ–‡å:")
            for en in sorted(ens):
                count = sum(pair_count[(e, cn)] for e in [en])
                display_en = f'"{en}"' if en else '(ç©º)'
                print(f"     - {display_en} ({count} ä¸ªè§’è‰²)")
            print()
    
    # 3. æ€»ä½“ç»Ÿè®¡
    print("=" * 80)
    print("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è§’è‰²æ•°: {len(data)}")
    print(f"ä¸åŒçš„è‹±æ–‡ä½œå“å: {len(en_to_cn)}")
    print(f"ä¸åŒçš„ä¸­æ–‡ä½œå“å: {len([cn for cn in cn_to_en.keys() if cn])}")
    print(f"ä¸åŒçš„(è‹±æ–‡,ä¸­æ–‡)ç»„åˆ: {len(pair_count)}")
    print(f"\néœ€è¦è§„èŒƒåŒ–çš„æ˜ å°„:")
    print(f"  - è‹±æ–‡åä¸ä¸€è‡´: {len(inconsistent_en)} ä¸ª")
    print(f"  - ä¸­æ–‡åä¸ä¸€è‡´: {len(inconsistent_cn)} ä¸ª")
    print(f"  - æ€»è®¡éœ€è¦å¤„ç†: {len(inconsistent_en) + len(inconsistent_cn)} ä¸ª")
    print("=" * 80)
    
    # 4. åˆ—å‡ºä¸€äº›å…·ä½“çš„ Fate ç³»åˆ—ä¾‹å­
    print("\nğŸ¯ Fate ç³»åˆ—çš„æ˜ å°„æƒ…å†µ:")
    fate_mappings = {}
    for (en, cn), count in pair_count.items():
        if 'fate' in en.lower() or 'fate' in cn.lower() or 'å‘½è¿' in cn:
            if en not in fate_mappings:
                fate_mappings[en] = []
            fate_mappings[en].append((cn, count))
    
    for en in sorted(fate_mappings.keys()):
        print(f"\n  {en}:")
        for cn, count in sorted(fate_mappings[en], key=lambda x: -x[1]):
            display_cn = f'"{cn}"' if cn else '(ç©º)'
            print(f"    - {display_cn}: {count} ä¸ªè§’è‰²")

if __name__ == '__main__':
    analyze_source_mappings()
