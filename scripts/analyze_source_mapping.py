import json
import os
from collections import defaultdict

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, '..', 'output', 'noob_characters-chants-en-cn.json')
MAPPING_FILE = os.path.join(BASE_DIR, 'source_name_mapping.json')

def analyze_source_mappings():
    """åˆ†æä½œå“åç§°çš„æ˜ å°„å…³ç³»ï¼Œæ‰¾å‡ºä¸ä¸€è‡´çš„æƒ…å†µï¼Œå¹¶ç”Ÿæˆå»ºè®®çš„è§„èŒƒåŒ–è§„åˆ™"""
    
    # è¯»å–æ•°æ®
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {DATA_FILE}")
        print("ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œ generate_cards_data_async.py ç”Ÿæˆæ•°æ®")
        return
    
    # è¯»å–ç°æœ‰çš„æ˜ å°„è§„åˆ™
    existing_mappings = {}
    try:
        with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
            existing_mappings = mapping_data.get('mappings', {})
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Š: æ˜ å°„è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {MAPPING_FILE}")
    
    # ç»Ÿè®¡ä¸åŒçš„æ˜ å°„å…³ç³»
    # 1. source_en -> set of source_cn (ä¸€ä¸ªè‹±æ–‡åå¯¹åº”å¤šä¸ªä¸­æ–‡å)
    en_to_cn = defaultdict(set)
    # 2. source_cn -> set of source_en (ä¸€ä¸ªä¸­æ–‡åå¯¹åº”å¤šä¸ªè‹±æ–‡å)
    cn_to_en = defaultdict(set)
    # 3. (source_en, source_cn) ç»„åˆçš„è§’è‰²æ•°é‡
    pair_count = defaultdict(int)
    # 4. ç©ºå€¼ç»Ÿè®¡
    empty_en_count = 0
    empty_cn_count = 0
    both_empty_count = 0
    
    for item in data:
        source_en = item.get('source_en', '').strip()
        source_cn = item.get('source_cn', '').strip()
        
        # ç»Ÿè®¡ç©ºå€¼
        if not source_en and not source_cn:
            both_empty_count += 1
            continue
        elif not source_en:
            empty_en_count += 1
        elif not source_cn:
            empty_cn_count += 1
        
        if source_en:
            en_to_cn[source_en].add(source_cn)
        if source_cn:
            cn_to_en[source_cn].add(source_en)
        
        if source_en or source_cn:
            pair_count[(source_en, source_cn)] += 1
    
    # æ‰¾å‡ºä¸ä¸€è‡´çš„æ˜ å°„
    print("=" * 80)
    print("ğŸ“Š ä½œå“åç§°æ˜ å°„åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # 0. æ•°æ®è´¨é‡ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ•°æ®è´¨é‡ç»Ÿè®¡")
    print(f"  æ€»è§’è‰²æ•°: {len(data)}")
    print(f"  è‹±æ–‡åä¸ºç©º: {empty_en_count} ä¸ª ({empty_en_count/len(data)*100:.1f}%)")
    print(f"  ä¸­æ–‡åä¸ºç©º: {empty_cn_count} ä¸ª ({empty_cn_count/len(data)*100:.1f}%)")
    print(f"  è‹±æ–‡+ä¸­æ–‡å‡ä¸ºç©º: {both_empty_count} ä¸ª ({both_empty_count/len(data)*100:.1f}%)")
    
    # 1. ä¸€ä¸ªè‹±æ–‡åå¯¹åº”å¤šä¸ªä¸­æ–‡åçš„æƒ…å†µ
    inconsistent_en = {en: cns for en, cns in en_to_cn.items() if len(cns) > 1}
    print(f"\nğŸ”´ ä¸€ä¸ªè‹±æ–‡ä½œå“åå¯¹åº”å¤šä¸ªä¸­æ–‡åçš„æƒ…å†µ: {len(inconsistent_en)} ä¸ª\n")
    
    if inconsistent_en:
        # æŒ‰ä¸ä¸€è‡´ç¨‹åº¦æ’åºï¼ˆä¸­æ–‡åæ•°é‡è¶Šå¤šè¶Šä¸¥é‡ï¼‰
        sorted_inconsistent = sorted(inconsistent_en.items(), key=lambda x: len(x[1]), reverse=True)
        
        for en, cns in sorted_inconsistent[:20]:
            # è®¡ç®—æ€»è§’è‰²æ•°
            total_chars = sum(pair_count[(en, cn)] for cn in cns)
            print(f"  ã€{en}ã€‘{len(cns)} ä¸ªä¸­æ–‡åï¼Œå…± {total_chars} ä¸ªè§’è‰²:")
            
            # æŒ‰è§’è‰²æ•°é‡æ’åºï¼Œæ˜¾ç¤ºå“ªä¸ªæ˜¯æœ€å¸¸ç”¨çš„
            cn_with_count = [(cn, pair_count[(en, cn)]) for cn in cns]
            cn_with_count.sort(key=lambda x: x[1], reverse=True)
            
            for cn, count in cn_with_count:
                display_cn = f'"{cn}"' if cn else '(ç©º)'
                percentage = count / total_chars * 100
                marker = "ğŸ‘‘" if count == cn_with_count[0][1] else "  "
                print(f"     {marker} {display_cn}: {count} ä¸ªè§’è‰² ({percentage:.1f}%)")
            print()
    
    # 2. ä¸€ä¸ªä¸­æ–‡åå¯¹åº”å¤šä¸ªè‹±æ–‡åçš„æƒ…å†µ
    inconsistent_cn = {cn: ens for cn, ens in cn_to_en.items() if len(ens) > 1 and cn}
    print(f"\nğŸŸ¡ ä¸€ä¸ªä¸­æ–‡ä½œå“åå¯¹åº”å¤šä¸ªè‹±æ–‡åçš„æƒ…å†µ: {len(inconsistent_cn)} ä¸ª\n")
    
    if inconsistent_cn:
        sorted_inconsistent = sorted(inconsistent_cn.items(), key=lambda x: len(x[1]), reverse=True)
        
        for cn, ens in sorted_inconsistent[:20]:
            total_chars = sum(pair_count[(en, cn)] for en in ens)
            print(f"  ã€{cn}ã€‘{len(ens)} ä¸ªè‹±æ–‡åï¼Œå…± {total_chars} ä¸ªè§’è‰²:")
            
            en_with_count = [(en, pair_count[(en, cn)]) for en in ens]
            en_with_count.sort(key=lambda x: x[1], reverse=True)
            
            for en, count in en_with_count:
                display_en = f'"{en}"' if en else '(ç©º)'
                percentage = count / total_chars * 100
                marker = "ğŸ‘‘" if count == en_with_count[0][1] else "  "
                print(f"     {marker} {display_en}: {count} ä¸ªè§’è‰² ({percentage:.1f}%)")
            print()
    
    # 3. æ£€æŸ¥å“ªäº›ä¸ä¸€è‡´çš„æ˜ å°„è¿˜æ²¡æœ‰è¢«è§„èŒƒåŒ–è§„åˆ™è¦†ç›–
    print(f"\nğŸ” æœªè¢«æ˜ å°„è¡¨è¦†ç›–çš„ä¸ä¸€è‡´æƒ…å†µ\n")
    
    en_rules = existing_mappings.get('english_normalization', {}).get('rules', {})
    cn_rules = existing_mappings.get('chinese_normalization', {}).get('rules', {})
    standard_pairs = existing_mappings.get('standard_pairs', {}).get('pairs', {})
    
    uncovered_en = []
    uncovered_cn = []
    
    # æ£€æŸ¥è‹±æ–‡åä¸ä¸€è‡´ä¸”æœªè¢«è¦†ç›–
    for en, cns in inconsistent_en.items():
        # æ£€æŸ¥è¿™ä¸ªè‹±æ–‡åæ˜¯å¦åœ¨ä»»ä½•è§„åˆ™ä¸­
        is_covered = False
        for standard_name, variants in en_rules.items():
            if en in variants or en == standard_name:
                is_covered = True
                break
        
        if not is_covered:
            total_chars = sum(pair_count[(en, cn)] for cn in cns)
            uncovered_en.append((en, cns, total_chars))
    
    # æ£€æŸ¥ä¸­æ–‡åä¸ä¸€è‡´ä¸”æœªè¢«è¦†ç›–
    for cn, ens in inconsistent_cn.items():
        is_covered = False
        for standard_name, variants in cn_rules.items():
            if cn in variants or cn == standard_name:
                is_covered = True
                break
        
        if not is_covered:
            total_chars = sum(pair_count[(en, cn)] for en in ens)
            uncovered_cn.append((cn, ens, total_chars))
    
    # æŒ‰å½±å“è§’è‰²æ•°æ’åº
    uncovered_en.sort(key=lambda x: x[2], reverse=True)
    uncovered_cn.sort(key=lambda x: x[2], reverse=True)
    
    if uncovered_en:
        print(f"  âš ï¸ è‹±æ–‡åæœªè¦†ç›–: {len(uncovered_en)} ä¸ª\n")
        for en, cns, total_chars in uncovered_en[:10]:
            print(f"    ã€{en}ã€‘å½±å“ {total_chars} ä¸ªè§’è‰²")
            for cn in sorted(cns):
                count = pair_count[(en, cn)]
                display_cn = f'"{cn}"' if cn else '(ç©º)'
                print(f"       -> {display_cn} ({count})")
            print()
    else:
        print("  âœ… æ‰€æœ‰è‹±æ–‡åä¸ä¸€è‡´é—®é¢˜å‡å·²è¢«æ˜ å°„è¡¨è¦†ç›–")
    
    if uncovered_cn:
        print(f"  âš ï¸ ä¸­æ–‡åæœªè¦†ç›–: {len(uncovered_cn)} ä¸ª\n")
        for cn, ens, total_chars in uncovered_cn[:10]:
            print(f"    ã€{cn}ã€‘å½±å“ {total_chars} ä¸ªè§’è‰²")
            for en in sorted(ens):
                count = pair_count[(en, cn)]
                display_en = f'"{en}"' if en else '(ç©º)'
                print(f"       -> {display_en} ({count})")
            print()
    else:
        print("  âœ… æ‰€æœ‰ä¸­æ–‡åä¸ä¸€è‡´é—®é¢˜å‡å·²è¢«æ˜ å°„è¡¨è¦†ç›–")
    
    # 4. æ€»ä½“ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è§’è‰²æ•°: {len(data)}")
    print(f"ä¸åŒçš„è‹±æ–‡ä½œå“å: {len(en_to_cn)}")
    print(f"ä¸åŒçš„ä¸­æ–‡ä½œå“å: {len([cn for cn in cn_to_en.keys() if cn])}")
    print(f"ä¸åŒçš„(è‹±æ–‡,ä¸­æ–‡)ç»„åˆ: {len(pair_count)}")
    print(f"\néœ€è¦è§„èŒƒåŒ–çš„æ˜ å°„:")
    print(f"  - è‹±æ–‡åä¸ä¸€è‡´: {len(inconsistent_en)} ä¸ª")
    print(f"  - ä¸­æ–‡åä¸ä¸€è‡´: {len(inconsistent_cn)} ä¸ª")
    print(f"  - æ€»è®¡éœ€è¦å¤„ç†: {len(inconsistent_en) + len(inconsistent_cn)} ä¸ª")
    print(f"\næ˜ å°„è¡¨è¦†ç›–æƒ…å†µ:")
    print(f"  - è‹±æ–‡è§„åˆ™æ•°: {len(en_rules)}")
    print(f"  - ä¸­æ–‡è§„åˆ™æ•°: {len(cn_rules)}")
    print(f"  - æ ‡å‡†é…å¯¹æ•°: {len(standard_pairs)}")
    print(f"  - æœªè¦†ç›–çš„è‹±æ–‡ä¸ä¸€è‡´: {len(uncovered_en)} ä¸ª")
    print(f"  - æœªè¦†ç›–çš„ä¸­æ–‡ä¸ä¸€è‡´: {len(uncovered_cn)} ä¸ª")
    
    coverage_rate = (1 - (len(uncovered_en) + len(uncovered_cn)) / (len(inconsistent_en) + len(inconsistent_cn))) * 100 if (len(inconsistent_en) + len(inconsistent_cn)) > 0 else 100
    print(f"  - è¦†ç›–ç‡: {coverage_rate:.1f}%")
    print("=" * 80)
    
    # 5. ç”Ÿæˆå»ºè®®çš„æ˜ å°„è§„åˆ™ï¼ˆå¦‚æœæœ‰æœªè¦†ç›–çš„ï¼‰
    if uncovered_en or uncovered_cn:
        print(f"\nğŸ’¡ å»ºè®®æ·»åŠ åˆ° source_name_mapping.json çš„è§„åˆ™:\n")
        
        if uncovered_en:
            print("  è‹±æ–‡è§„èŒƒåŒ–å»ºè®®ï¼ˆæ·»åŠ åˆ° english_normalization.rulesï¼‰:")
            for en, cns, total_chars in uncovered_en[:5]:
                print(f"    \"{en}\": [")
                print(f"        \"{en}\"")
                print(f"    ],")
            print()
        
        if uncovered_cn:
            print("  ä¸­æ–‡è§„èŒƒåŒ–å»ºè®®ï¼ˆæ·»åŠ åˆ° chinese_normalization.rulesï¼‰:")
            for cn, ens, total_chars in uncovered_cn[:5]:
                print(f"    \"{cn}\": [")
                print(f"        \"{cn}\"")
                print(f"    ],")
            print()

if __name__ == '__main__':
    analyze_source_mappings()
