import asyncio
import json
import os
import aiohttp
import time
import random
import argparse
from typing import List, Dict
from tqdm.asyncio import tqdm
import os
import sys
from dotenv import load_dotenv

load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ================= é…ç½®åŠ è½½ =================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = os.path.join(BASE_DIR, 'config.json')
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Š: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None

# åŠ è½½é…ç½®
CONFIG = load_config()

# ================= é…ç½®é¡¹ =================
# ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶åˆ™ä½¿ç”¨é»˜è®¤å€¼
if CONFIG:
    # LLM é…ç½®
    BATCH_SIZE = CONFIG['llm'].get('batch_size', 10)
    LLM_CONCURRENCY = CONFIG['llm'].get('concurrency', 5)
    LLM_RETRY_TIMES = CONFIG['llm'].get('retry_times', 3)
    LLM_RETRY_DELAY = CONFIG['llm'].get('retry_delay', 2)
    
    # å›¾ç‰‡é…ç½®
    IMG_CONCURRENCY = CONFIG['image'].get('concurrency', 10)
    IMG_RETRY_TIMES = CONFIG['image'].get('retry_times', 2)
    IMG_RETRY_DELAY = CONFIG['image'].get('retry_delay', 1)
    
    # å¤„ç†é…ç½®
    SAVE_INTERVAL_BATCHES = CONFIG['processing'].get('save_interval_batches', 5)
    
    # è·¯å¾„é…ç½®
    INPUT_URL = CONFIG['paths'].get('input_url')
    OUTPUT_FILE = os.path.join(BASE_DIR, CONFIG['paths'].get('output_file'))
    DEBUG_OUTPUT_FILE = os.path.join(BASE_DIR, CONFIG['paths'].get('debug_output_file'))
    DATA_DIR = os.path.join(BASE_DIR, CONFIG['paths'].get('data_dir'))
    CACHED_SOURCE_FILE = os.path.join(BASE_DIR, CONFIG['paths'].get('cached_source_file'))
    MAPPING_FILE = os.path.join(BASE_DIR, CONFIG['paths'].get('mapping_file'))
else:
    # é»˜è®¤é…ç½®
    BATCH_SIZE = 10
    LLM_CONCURRENCY = 5
    LLM_RETRY_TIMES = 3
    LLM_RETRY_DELAY = 2
    IMG_CONCURRENCY = 10
    IMG_RETRY_TIMES = 2
    IMG_RETRY_DELAY = 1
    SAVE_INTERVAL_BATCHES = 5
    
    INPUT_URL = "https://raw.githubusercontent.com/DominikDoom/a1111-sd-webui-tagcomplete/refs/heads/main/tags/noob_characters-chants.json"
    OUTPUT_FILE = os.path.join(BASE_DIR, '..', 'output', 'noob_characters-chants-en-cn.json')
    DEBUG_OUTPUT_FILE = os.path.join(BASE_DIR, '..', 'output', 'debug_output.json')
    DATA_DIR = os.path.join(BASE_DIR, '..', 'data')
    CACHED_SOURCE_FILE = os.path.join(DATA_DIR, 'noob_characters-chants.json')
    MAPPING_FILE = os.path.join(BASE_DIR, 'source_name_mapping.json')

# --- LLM API é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰ ---
LLM_API_URL = os.getenv("LLM_API_URL") 
LLM_API_KEY = os.getenv("LLM_API_KEY") 
LLM_MODEL = os.getenv("LLM_MODEL")

# å…¨å±€ä¿¡å·é‡ï¼ˆå°†åœ¨ main ä¸­æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆå§‹åŒ–ï¼‰
sem_llm = None
sem_img = None

# å…¨å±€æ˜ å°„è¡¨ï¼ˆå°†åœ¨ main ä¸­åŠ è½½ï¼‰
source_name_mapping = None



# å…¨å±€ç»Ÿè®¡å¯¹è±¡
class Stats:
    """æ€§èƒ½ç»Ÿè®¡ç±»"""
    def __init__(self):
        self.llm_success = 0
        self.llm_fail = 0
        self.img_success = 0
        self.img_fail = 0
        self.total_processed = 0
        self.start_time = time.time()
    
    def print_summary(self):
        duration = time.time() - self.start_time
        llm_total = self.llm_success + self.llm_fail
        img_total = self.img_success + self.img_fail
        
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        print("="*50)
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ¯ æ€»å¤„ç†: {self.total_processed} ä¸ªè§’è‰²")
        if duration > 0:
            print(f"âš¡ å¹³å‡é€Ÿåº¦: {self.total_processed / duration:.2f} ä¸ª/ç§’")
        print(f"\nğŸ¤– LLM ç¿»è¯‘:")
        if llm_total > 0:
            print(f"   âœ… æˆåŠŸ: {self.llm_success}/{llm_total} ({self.llm_success/llm_total*100:.1f}%)")
            print(f"   âŒ å¤±è´¥: {self.llm_fail}/{llm_total} ({self.llm_fail/llm_total*100:.1f}%)")
        print(f"\nğŸ–¼ï¸  å›¾ç‰‡æœç´¢:")
        if img_total > 0:
            print(f"   âœ… æˆåŠŸ: {self.img_success}/{img_total} ({self.img_success/img_total*100:.1f}%)")
            print(f"   âŒ å¤±è´¥: {self.img_fail}/{img_total} ({self.img_fail/img_total*100:.1f}%)")
        print("="*50)

stats = Stats()

def load_source_name_mapping(mapping_file: str) -> Dict:
    """
    åŠ è½½ä½œå“åç§°è§„èŒƒåŒ–æ˜ å°„è¡¨
    
    Returns:
        åŒ…å«è§„èŒƒåŒ–è§„åˆ™çš„å­—å…¸
    """
    try:
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        print(f"âœ… å·²åŠ è½½ä½œå“åç§°æ˜ å°„è¡¨: {mapping_file}")
        return mapping
    except FileNotFoundError:
        print(f"âš ï¸  è­¦å‘Š: æ˜ å°„è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file}")
        return None
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: åŠ è½½æ˜ å°„è¡¨å¤±è´¥: {e}")
        return None

def normalize_source_names(source_en: str, source_cn: str) -> tuple:
    """
    è§„èŒƒåŒ–ä½œå“è‹±æ–‡åå’Œä¸­æ–‡å
    
    Args:
        source_en: åŸå§‹è‹±æ–‡ä½œå“å
        source_cn: åŸå§‹ä¸­æ–‡ä½œå“å
    
    Returns:
        (è§„èŒƒåŒ–åçš„è‹±æ–‡å, è§„èŒƒåŒ–åçš„ä¸­æ–‡å)
    """
    if not source_name_mapping:
        return source_en, source_cn
    
    mappings = source_name_mapping.get('mappings', {})
    en_rules = mappings.get('english_normalization', {}).get('rules', {})
    cn_rules = mappings.get('chinese_normalization', {}).get('rules', {})
    standard_pairs = mappings.get('standard_pairs', {}).get('pairs', {})
    
    normalized_en = source_en
    normalized_cn = source_cn
    
    # 1. å…ˆå°è¯•è§„èŒƒåŒ–è‹±æ–‡å
    for standard_en, variants in en_rules.items():
        if source_en in variants:
            normalized_en = standard_en
            break
    
    # 2. å†å°è¯•è§„èŒƒåŒ–ä¸­æ–‡å
    for standard_cn, variants in cn_rules.items():
        if source_cn in variants:
            normalized_cn = standard_cn
            break
    
    # 3. å¦‚æœè‹±æ–‡åæ˜¯æ ‡å‡†çš„ï¼Œä¸”ä¸­æ–‡åä¸ºç©ºæˆ–ä¸æ ‡å‡†ï¼Œä½¿ç”¨æ ‡å‡†é…å¯¹
    if normalized_en in standard_pairs and (not normalized_cn or normalized_cn != standard_pairs[normalized_en]):
        normalized_cn = standard_pairs[normalized_en]
    
    # 4. å¦‚æœä¸­æ–‡åæ˜¯æ ‡å‡†çš„ï¼Œä¸”è‹±æ–‡åä¸ºç©ºæˆ–ä¸æ ‡å‡†ï¼Œåå‘æŸ¥æ‰¾æ ‡å‡†é…å¯¹
    if normalized_cn:
        for std_en, std_cn in standard_pairs.items():
            if normalized_cn == std_cn and (not normalized_en or normalized_en != std_en):
                normalized_en = std_en
                break
    
    return normalized_en, normalized_cn





async def call_llm_custom(session: aiohttp.ClientSession, prompt: str) -> str:
    """è°ƒç”¨ LLM æ¥å£è·å–å…ƒæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": "You are a JSON generator helper."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 65536,
        "thinking": {
            "type": "disabled"
        },
        "temperature": 0.6,
        "top_p": 0.95,
        "response_format": {"type": "json_object"}
    }

    # é‡è¯•é€»è¾‘
    for attempt in range(LLM_RETRY_TIMES):
        try:
            async with sem_llm: # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶ LLM å¹¶å‘
                async with session.post(LLM_API_URL, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        # ç»Ÿè®¡å·²ç§»è‡³ translate_batch_task æŒ‰è§’è‰²æ•°é‡ç»Ÿè®¡
                        return result['choices'][0]['message']['content']
                    else:
                        # æ‰“å°é”™è¯¯çŠ¶æ€ç ï¼Œæ–¹ä¾¿è°ƒè¯•
                        if attempt == LLM_RETRY_TIMES - 1:
                            print(f"\n[LLM Error] Status: {response.status} (å·²é‡è¯•{attempt+1}æ¬¡)")
        except Exception as e:
            if attempt == LLM_RETRY_TIMES - 1:
                print(f"\n[LLM] è¯·æ±‚å¼‚å¸¸: {e} (å·²é‡è¯•{attempt+1}æ¬¡)")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < LLM_RETRY_TIMES - 1:
            await asyncio.sleep(LLM_RETRY_DELAY * (attempt + 1))  # æŒ‡æ•°é€€é¿
    
    # ç»Ÿè®¡å¤±è´¥å·²ç§»è‡³ translate_batch_task
    return None

async def search_image_safebooru(session: aiohttp.ClientSession, tag: str) -> str:
    """Safebooru æœå›¾ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    url = f"https://safebooru.org/index.php?page=dapi&s=post&q=index&tags={tag}+solo&limit=1&json=1"
    
    # é‡è¯•é€»è¾‘
    for attempt in range(IMG_RETRY_TIMES):
        try:
            # ä½¿ç”¨å…¨å±€ä¿¡å·é‡é™åˆ¶å›¾ç‰‡å¹¶å‘
            async with sem_img:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        data = await resp.json(content_type=None)
                        if data and isinstance(data, list) and len(data) > 0:
                            img = data[0]
                            stats.img_success += 1
                            return f"https://safebooru.org/images/{img['directory']}/{img['image']}"
        except Exception:
            pass
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < IMG_RETRY_TIMES - 1:
            await asyncio.sleep(IMG_RETRY_DELAY)
    
    stats.img_fail += 1
    return None

async def process_images_for_list(session: aiohttp.ClientSession, data_list: List[Dict]) -> List[Dict]:
    """ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¡ç›®å¹¶å‘è·å–å›¾ç‰‡"""
    tasks = []
    
    async def _task(item):
        # å¦‚æœå·²ç»æœ‰å›¾ï¼Œç›´æ¥è¿”å›
        if item.get('image_url') and str(item['image_url']).startswith('http'):
            return item
        
        # æ²¡å›¾åˆ™å»æœ
        img_url = await search_image_safebooru(session, item['tag'])
        item['image_url'] = img_url
        
        # ç®€å•çš„æ—¥å¿—è¾“å‡º (å¯é€‰ï¼Œé«˜å¹¶å‘ä¸‹åˆ·å±å¯æ³¨é‡Šæ‰)
        # log_name = item.get('cn_name') or item['tag']
        # status = "âœ…" if img_url else "âŒ"
        # print(f"{status} {log_name}", end="\r") 
        
        return item

    for item in data_list:
        tasks.append(_task(item))
    
    # ç­‰å¾…è¯¥æ‰¹æ¬¡æ‰€æœ‰å›¾ç‰‡ä»»åŠ¡å®Œæˆ
    return await asyncio.gather(*tasks)

async def translate_batch_task(session: aiohttp.ClientSession, batch_data: List[Dict]) -> List[Dict]:
    """
    LLM ç¿»è¯‘ä»»åŠ¡
    """
    
    # æå–è¦ç¿»è¯‘çš„ tag åˆ—è¡¨
    tags_to_translate = [item['tag'] for item in batch_data]
    tags_str = '\n'.join([f"{i+1}. {tag}" for i, tag in enumerate(tags_to_translate)])
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šACGæ–‡åŒ–çš„ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ {len(batch_data)} ä¸ª Danbooru Character Tags ç¿»è¯‘æˆ JSON æ ¼å¼ã€‚

    **è¦ç¿»è¯‘çš„è§’è‰²æ ‡ç­¾**ï¼š
{tags_str}

    **ç¿»è¯‘è¦æ±‚**:
    1. å¿…é¡»è¿”å› {len(batch_data)} ä¸ªå¯¹è±¡ï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    2. æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
       - "tag": åŸæ ‡ç­¾ï¼ˆä»ä¸Šé¢åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¿æŒä¸å˜ï¼‰
       - "cn_name": ä¸­æ–‡è§’è‰²åï¼ˆå¦‚æœæ— æ³•ç¡®å®šï¼Œç•™ç©ºï¼‰
       - "cn_name_status": ä¸­æ–‡åçŠ¶æ€ï¼ˆå®˜æ–¹è¯‘å/æ¨æ–­è¯‘å/æœªçŸ¥ï¼‰
       - "en_name": è‹±æ–‡è§’è‰²åï¼ˆå»æ‰ä¸‹åˆ’çº¿ï¼Œé¦–å­—æ¯å¤§å†™ï¼‰
       - "source_cn": ä½œå“ä¸­æ–‡åï¼ˆå¦‚æœæ— æ³•ç¡®å®šï¼Œç•™ç©ºï¼‰
       - "source_en": ä½œå“è‹±æ–‡å
       - "source_name_status": ä½œå“åçŠ¶æ€ï¼ˆå®˜æ–¹è¯‘å/æ¨æ–­è¯‘å/æœªçŸ¥ï¼‰

    3. **æ‹¬å·å¤„ç†è§„åˆ™**ï¼š
       å¦‚æœ tag ä¸­åŒ…å«æ‹¬å·ï¼Œä¾‹å¦‚ character_(xxx)ï¼Œè¯·æŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†ï¼š
       
       - å¦‚æœæ˜¯**æœè£…/å½¢æ€/ç‰ˆæœ¬**æè¿°ï¼ˆå¦‚ 1st_costume, 2nd_costume, summer, winter, casual, maid, racing, idol ç­‰ï¼‰ï¼š
         * åœ¨ cn_name ä¸­æ·»åŠ å¯¹åº”çš„ä¸­æ–‡æè¿°ï¼Œæ ¼å¼ï¼šè§’è‰²åï¼ˆæè¿°ï¼‰
         * åœ¨ en_name ä¸­ä¹Ÿä¿ç•™æ‹¬å·ï¼Œæ ¼å¼ï¼šCharacter Name (Description)
         ä¾‹å¦‚ï¼šinuyama_tamaki_(1st_costume) â†’ cn_name: "çŠ¬å±±ç‰å§¬ï¼ˆç¬¬ä¸€å¥—æœè£…ï¼‰", en_name: "Inuyama Tamaki (1st Costume)"
       
       - å¦‚æœæ˜¯**ä½œå“åç§°**ï¼ˆç”¨äºåŒºåˆ†åŒåè§’è‰²ï¼Œå¦‚ touhou, fate, pokemon ç­‰ï¼‰ï¼š
         * cn_name å’Œ en_name **ä¸åŒ…å«æ‹¬å·å’Œä½œå“å**ï¼Œåªå†™è§’è‰²å
         * å°†æ‹¬å·å†…çš„ä½œå“åæå–åˆ° source_cn å’Œ source_en
         ä¾‹å¦‚ï¼šringo_(touhou) â†’ cn_name: "é“ƒç‘š", en_name: "Ringo", source_cn: "ä¸œæ–¹Project", source_en: "Touhou Project"
         ä¾‹å¦‚ï¼šsakura_(cardcaptor_sakura) â†’ cn_name: "å°æ¨±", en_name: "Sakura", source_cn: "é­”å¡å°‘å¥³æ¨±", source_en: "Cardcaptor Sakura"


    4. ä¸¥ç¦ä½¿ç”¨ Markdown ä»£ç å—åŒ…è£¹ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„

    è¯·ç¿»è¯‘ä»¥ä¸Š {len(batch_data)} ä¸ªæ ‡ç­¾ï¼Œç¡®ä¿è¿”å›æ•°é‡æ­£ç¡®ã€‚
    """
    
    content = await call_llm_custom(session, prompt)
    
    # æ„é€ é»˜è®¤è¿”å›å€¼ï¼Œé˜²æ­¢ LLM æŒ‚äº†å¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡ä¸¢å¤±
    # åŒæ—¶ä¿ç•™åŸå§‹çš„ color å’Œ content å­—æ®µ
    # LLMé”™è¯¯æ—¶ cn_name_status ç•™ç©ºï¼Œæ–¹ä¾¿åŒºåˆ†çœŸæ­£çš„"æœªçŸ¥"çŠ¶æ€
    default_res = [
        {
            "tag": item['tag'], 
            "cn_name": "", 
            "cn_name_status": "",  # LLMé”™è¯¯æ—¶ç•™ç©º
            "en_name": item['tag'], 
            "source_cn": "", 
            "source_en": "",
            "source_name_status": "",  # LLMé”™è¯¯æ—¶ç•™ç©º
            "color": item['color'],
            "content": item['content']
        } 
        for item in batch_data
    ]

    if not content:
        print("\nâš ï¸ LLM è¿”å›å†…å®¹ä¸ºç©º")
        # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
        stats.llm_fail += len(batch_data)
        return default_res

    try:
        clean_content = content.replace("```json", "").replace("```", "").strip()
        result = json.loads(clean_content)
        
        # å…¼å®¹ LLM å¯èƒ½è¿”å› {"items": [...]} æˆ–ç›´æ¥ [...] çš„æƒ…å†µ
        items = None
        if isinstance(result, dict):
            for val in result.values():
                if isinstance(val, list): 
                    items = val
                    break
        elif isinstance(result, list):
            items = result
        
        if not items:
            print("\nâš ï¸ æ— æ³•ä» LLM è¿”å›ä¸­æå–åˆ—è¡¨æ•°æ®")
            # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
            stats.llm_fail += len(batch_data)
            return default_res
        
        # ä¿®å¤ï¼šéªŒè¯å¹¶å¤„ç† LLM è¿”å›æ•°é‡é—®é¢˜
        if len(items) != len(batch_data):
            if len(items) > len(batch_data):
                # è¿”å›æ•°é‡è¿‡å¤šï¼Œæˆªæ–­
                print(f"\nâš ï¸ è­¦å‘Š: LLM è¿”å› {len(items)} é¡¹ï¼Œè¶…è¿‡æ‰¹æ¬¡å¤§å° {len(batch_data)}ï¼Œæˆªæ–­å¤šä½™é¡¹")
                items = items[:len(batch_data)]
            else:
                # è¿”å›æ•°é‡ä¸è¶³
                missing_count = len(batch_data) - len(items)
                print(f"\nâš ï¸ è­¦å‘Š: LLM è¿”å› {len(items)} é¡¹ï¼Œç¼ºå°‘ {missing_count} é¡¹")
                
                # å¦‚æœç¼ºå¤±è¿‡å¤šï¼ˆè¶…è¿‡50%ï¼‰ï¼Œè®¤ä¸ºå¤±è´¥
                if len(items) < len(batch_data) * 0.5:
                    print(f"  â†’ è¿”å›æ•°é‡å¤ªå°‘ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                    stats.llm_fail += len(batch_data)
                    return default_res
                else:
                    # åªç¼ºå°‘ä¸€ç‚¹ï¼Œè¡¥å……é»˜è®¤å€¼
                    print(f"  â†’ è¡¥å……ç¼ºå¤±çš„ {missing_count} é¡¹")
                    
                    # è·å–å·²è¿”å›çš„ tag
                    returned_tags = {item.get('tag') for item in items}
                    
                    # ä¸ºç¼ºå¤±çš„æ¡ç›®æ·»åŠ é»˜è®¤å€¼
                    for item in batch_data:
                        if item['tag'] not in returned_tags:
                            items.append({
                                "tag": item['tag'],
                                "cn_name": "",
                                "cn_name_status": "",
                                "en_name": item['tag'],
                                "source_cn": "",
                                "source_en": "",
                                "color": item['color'],
                                "content": item['content']
                            })
                    
                    # éƒ¨åˆ†æˆåŠŸï¼Œéƒ¨åˆ†å¤±è´¥
                    stats.llm_success += len(items) - missing_count
                    stats.llm_fail += missing_count
        
        # å°† color å’Œ content å­—æ®µåˆå¹¶åˆ° LLM è¿”å›çš„ç»“æœä¸­
        tag_to_data = {item['tag']: item for item in batch_data}
        for item in items:
            tag = item.get('tag')
            if tag and tag in tag_to_data:
                item['color'] = tag_to_data[tag]['color']
                item['content'] = tag_to_data[tag]['content']
            
            # è§„èŒƒåŒ–ä½œå“åç§°
            source_en = item.get('source_en', '')
            source_cn = item.get('source_cn', '')
            normalized_en, normalized_cn = normalize_source_names(source_en, source_cn)
            item['source_en'] = normalized_en
            item['source_cn'] = normalized_cn
        
        # ç»Ÿè®¡æˆåŠŸçš„è§’è‰²æ•°é‡
        stats.llm_success += len(items)
        return items
    except Exception as e:
        print(f"\nâŒ LLM æ•°æ®è§£æå¼‚å¸¸: {e}")
        # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
        stats.llm_fail += len(batch_data)
        return default_res

async def pipeline_batch(session: aiohttp.ClientSession, batch_data: List[Dict]) -> List[Dict]:
    """
    å•ä¸ªæ‰¹æ¬¡çš„å®Œæ•´æµæ°´çº¿ï¼š
    1. ç­‰å¾… LLM ä¿¡å·é‡ -> è¯·æ±‚ LLM
    2. è·å–åˆ° JSON -> è¯·æ±‚ Images (å†…éƒ¨æœ‰ Image ä¿¡å·é‡)
    3. è¿”å›ç»“æœ
    
    Args:
        batch_data: åŒ…å« {"tag": str, "color": int, "content": str} çš„åˆ—è¡¨
    """
    # 1. LLM é˜¶æ®µ
    translated_items = await translate_batch_task(session, batch_data)
    
    # 2. æœå›¾é˜¶æ®µ
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦å†æ˜¾å¼åŠ é”ï¼Œå› ä¸º search_image_safebooru å†…éƒ¨æœ‰ sem_img æ§åˆ¶
    final_items = await process_images_for_list(session, translated_items)
    
    return final_items

def save_data(data: List[Dict], output_file: str = None):
    """è¾…åŠ©å‡½æ•°ï¼šä¿å­˜æ•°æ®åˆ°ç£ç›˜
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º OUTPUT_FILE
    """
    if output_file is None:
        output_file = OUTPUT_FILE
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")

# LLM é…ç½®å®Œæ•´æ€§æ£€æµ‹
def check_llm_config():
    # æ£€æŸ¥ URL æ˜¯å¦ä¸ºç©º
    if not LLM_API_URL or not LLM_API_URL.strip():
        print("\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ LLM_API_URL æœªé…ç½®ï¼")
        print("ğŸ’¡ æç¤ºï¼šè¯·è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œæˆ–ä½¿ç”¨ .env æ–‡ä»¶ã€‚")
        sys.exit(1)

    # æ£€æŸ¥ Key æ˜¯å¦ä¸ºç©º
    if not LLM_API_KEY or not LLM_API_KEY.strip():
        print("\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ LLM_API_KEY æœªé…ç½®ï¼")
        print("ğŸ’¡ æç¤ºï¼šè¯·è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ LLM_API_KEYã€‚")
        sys.exit(1)

def load_tags_from_file(filepath: str) -> Dict[str, Dict]:
    """
    ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è§’è‰²æ ‡ç­¾æ•°æ®
    
    Args:
        filepath: æœ¬åœ°JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        å­—å…¸ï¼Œkey ä¸ºè§’è‰² nameï¼Œvalue ä¸ºåŒ…å« color å’Œ content çš„å­—å…¸
        å¦‚æœåŠ è½½å¤±è´¥è¿”å›ç©ºå­—å…¸
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ä» JSON æ•°ç»„ä¸­æå– terms ä¸º "Character" çš„æ•°æ®
        if isinstance(data, list):
            tags_dict = {}
            for item in data:
                if item.get('name') and item.get('terms') == 'Character':
                    tags_dict[item['name']] = {
                        'color': item.get('color', 0),
                        'content': item.get('content', '')
                    }
            print(f"âœ… ä»ç¼“å­˜åŠ è½½ {len(tags_dict)} ä¸ªè§’è‰²æ ‡ç­¾")
            return tags_dict
        else:
            print(f"âš ï¸ è­¦å‘Š: ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            return {}
    except FileNotFoundError:
        return {}
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥ - {e}")
        return {}

async def fetch_tags_from_url(url: str, cache_file: str = None) -> Dict[str, Dict]:
    """
    ä»æŒ‡å®š URL è·å–è§’è‰²æ ‡ç­¾æ•°æ®ï¼Œå¹¶å¯é€‰åœ°ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
    
    Args:
        url: JSON æ•°æ®çš„ URL åœ°å€
        cache_file: å¯é€‰çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜åŸå§‹æ•°æ®åˆ°è¯¥æ–‡ä»¶
    
    Returns:
        å­—å…¸ï¼Œkey ä¸ºè§’è‰² nameï¼Œvalue ä¸ºåŒ…å« color å’Œ content çš„å­—å…¸
        æ ¼å¼: {"character_name": {"color": 4, "content": "..."}}
        å¦‚æœè·å–å¤±è´¥è¿”å›ç©ºå­—å…¸
    
    Note:
        æ•°æ®æ ¼å¼ç¤ºä¾‹:
        [
            {
                "name": "00_gundam",
                "terms": "Character",
                "content": "00 gundam,no humans, blue eyes...",
                "color": 4
            }
        ]
        åªæå– terms == "Character" çš„æ¡ç›®
    """
    print(f"ğŸ“¥ æ­£åœ¨ä» URL è·å–æ•°æ®: {url}")
    timeout = aiohttp.ClientTimeout(total=30)
    
    try:
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                if response.status != 200:
                    print(f"âŒ é”™è¯¯: æ— æ³•è·å–æ•°æ®ï¼ŒçŠ¶æ€ç : {response.status}")
                    return {}
                
                # GitHub raw æ–‡ä»¶è¿”å› text/plainï¼Œéœ€è¦å¿½ç•¥ Content-Type æ£€æŸ¥
                data = await response.json(content_type=None)
                
                # å¦‚æœæä¾›äº†ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜åŸå§‹æ•°æ®
                if cache_file:
                    try:
                        # ç¡®ä¿ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(cache_file), exist_ok=True)
                        with open(cache_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ç¼“å­˜è‡³: {cache_file}")
                    except Exception as e:
                        print(f"âš ï¸ è­¦å‘Š: ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥ - {e}")
                
                # ä» JSON æ•°ç»„ä¸­æå– terms ä¸º "Character" çš„æ•°æ®
                if isinstance(data, list):
                    # æ„å»ºå­—å…¸: {name: {color, content}}
                    tags_dict = {}
                    for item in data:
                        if item.get('name') and item.get('terms') == 'Character':
                            tags_dict[item['name']] = {
                                'color': item.get('color', 0),
                                'content': item.get('content', '')
                            }
                    print(f"âœ… æˆåŠŸè·å– {len(tags_dict)} ä¸ªè§’è‰²æ ‡ç­¾")
                    return tags_dict
                elif isinstance(data, dict):
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè¿”å›ç©ºï¼ˆä¸æ”¯æŒï¼‰
                    print(f"âš ï¸ è­¦å‘Š: æ•°æ®ä¸ºå­—å…¸æ ¼å¼ï¼Œè¯·æ£€æŸ¥ URL")
                    return {}
                else:
                    print(f"âŒ é”™è¯¯: æœªçŸ¥çš„æ•°æ®æ ¼å¼")
                    return {}
                
    except Exception as e:
        print(f"âŒ é”™è¯¯: è·å–æ•°æ®å¤±è´¥ - {e}")
        return {}

def apply_debug_filter(tags_dict: Dict[str, Dict], limit: int, random_sample: bool) -> Dict[str, Dict]:
    """
    åº”ç”¨æ•°é‡é™åˆ¶è¿‡æ»¤
    
    Args:
        tags_dict: å®Œæ•´çš„æ ‡ç­¾å­—å…¸
        limit: æ•°é‡é™åˆ¶ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        random_sample: æ˜¯å¦éšæœºæŠ½å–
    
    Returns:
        è¿‡æ»¤åçš„æ ‡ç­¾å­—å…¸
    """
    # å¦‚æœæœªè®¾ç½®é™åˆ¶ï¼Œè¿”å›å…¨éƒ¨
    if limit == 0:
        return tags_dict
    
    original_count = len(tags_dict)
    all_tags = list(tags_dict.keys())
    
    if random_sample:
        # éšæœºæŠ½å–
        selected_tags = random.sample(all_tags, min(limit, len(all_tags)))
        print(f"ğŸ” é™é‡æ¨¡å¼: éšæœºæŠ½å– {len(selected_tags)}/{original_count} æ¡æ•°æ®")
    else:
        # æŒ‰é¡ºåºå–å‰Næ¡
        selected_tags = all_tags[:limit]
        print(f"ğŸ” é™é‡æ¨¡å¼: å–å‰ {len(selected_tags)}/{original_count} æ¡æ•°æ®")
    
    # è¿”å›è¿‡æ»¤åçš„å­—å…¸
    return {tag: tags_dict[tag] for tag in selected_tags}



def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¼‚æ­¥æ‰¹é‡å¤„ç†åŠ¨æ¼«è§’è‰²æ•°æ®ï¼šLLMç¿»è¯‘ + å›¾ç‰‡æœç´¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  # åªå¤„ç†å‰10000ä¸ªè§’è‰²
  python %(prog)s --limit 10000
  
  # ç”Ÿäº§æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ•°æ®
  python %(prog)s
  
  # è‡ªå®šä¹‰å¹¶å‘æ•°
  python %(prog)s --llm-concurrency 10 --img-concurrency 20
        ''')
    
    # æ•°æ®å¤„ç†é€‰é¡¹
    parser.add_argument('--limit', type=int, default=0, 
                        help='é™åˆ¶å¤„ç†çš„æ•°æ®é‡ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œé»˜è®¤: 0ï¼‰ã€‚ä¾‹å¦‚ --limit 10000 åªå¤„ç†10000ä¸ªè§’è‰²')
    parser.add_argument('--random', action='store_true', 
                        help='éšæœºæŠ½å–æ•°æ®ï¼ˆé»˜è®¤ï¼šæŒ‰é¡ºåºï¼‰ã€‚éœ€é…åˆ --limit ä½¿ç”¨')
    parser.add_argument('--force-update', action='store_true',
                        help='å¼ºåˆ¶ä» URL é‡æ–°æ‹‰å–æºæ•°æ®ï¼ˆå¿½ç•¥æœ¬åœ°ç¼“å­˜ï¼‰')
    parser.add_argument('--debug', action='store_true',
                        help='Debug æ¨¡å¼ï¼šå¿½ç•¥å†å²æ•°æ®ï¼Œè¾“å‡ºåˆ° debug_output.jsonï¼Œä¸å½±å“æ­£å¼æ–‡ä»¶')
    
    # å¹¶å‘æ§åˆ¶
    parser.add_argument('--llm-concurrency', type=int, default=LLM_CONCURRENCY,
                        help=f'LLM å¹¶å‘æ•°ï¼ˆé»˜è®¤: {LLM_CONCURRENCY}ï¼‰')
    parser.add_argument('--img-concurrency', type=int, default=IMG_CONCURRENCY,
                        help=f'å›¾ç‰‡æœç´¢å¹¶å‘æ•°ï¼ˆé»˜è®¤: {IMG_CONCURRENCY}ï¼‰')
    
    # æ‰¹å¤„ç†é…ç½®
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                        help=f'æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤: {BATCH_SIZE}ï¼‰')
    
    return parser.parse_args()

async def main():
    global sem_llm, sem_img, source_name_mapping
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # åŠ è½½ä½œå“åç§°æ˜ å°„è¡¨
    source_name_mapping = load_source_name_mapping(MAPPING_FILE)
    
    # åˆå§‹åŒ–ä¿¡å·é‡
    sem_llm = asyncio.Semaphore(args.llm_concurrency)
    sem_img = asyncio.Semaphore(args.img_concurrency)
    
    check_llm_config()

    # 1. è¯»å–è¾“å…¥æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œé™¤éå¼ºåˆ¶æ›´æ–°ï¼‰
    tags_dict = {}
    
    if not args.force_update and os.path.exists(CACHED_SOURCE_FILE):
        # ä¼˜å…ˆä»ç¼“å­˜è¯»å–
        print(f"ğŸ“‚ å‘ç°æœ¬åœ°ç¼“å­˜æ–‡ä»¶: {CACHED_SOURCE_FILE}")
        tags_dict = load_tags_from_file(CACHED_SOURCE_FILE)
        
        if not tags_dict:
            print("âš ï¸ ç¼“å­˜æ–‡ä»¶æ— æ•ˆï¼Œå°è¯•ä» URL è·å–æ•°æ®")
            tags_dict = await fetch_tags_from_url(INPUT_URL, CACHED_SOURCE_FILE)
    else:
        # ä» URL è·å–æ•°æ®å¹¶ç¼“å­˜
        if args.force_update:
            print("ğŸ”„ å¼ºåˆ¶æ›´æ–°æ¨¡å¼ï¼šä» URL é‡æ–°æ‹‰å–æ•°æ®")
        else:
            print("ğŸ“¥ æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ï¼Œä» URL è·å–æ•°æ®")
        tags_dict = await fetch_tags_from_url(INPUT_URL, CACHED_SOURCE_FILE)
    
    if not tags_dict:
        print("âŒ é”™è¯¯: æ— æ³•è·å–æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®")
        return
    
    print(f"ğŸš€ è¾“å…¥æ€»æ•°: {len(tags_dict)}")
    
    # åº”ç”¨æ•°é‡é™åˆ¶è¿‡æ»¤
    tags_dict = apply_debug_filter(tags_dict, args.limit, args.random)
    
    # Debug æ¨¡å¼æç¤º
    if args.debug:
        print("\n" + "="*60)
        print("ğŸ› DEBUG æ¨¡å¼å·²å¯ç”¨")
        print("="*60)
        print("âš ï¸  æ­¤æ¨¡å¼å°†ï¼š")
        print("  1. å¿½ç•¥å†å²æ•°æ®ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰æŒ‡å®šçš„è§’è‰²")
        print("  2. è¾“å‡ºåˆ° debug_output.jsonï¼ˆä¸å½±å“æ­£å¼æ–‡ä»¶ï¼‰")
        print("  3. æ¯æ¬¡è¿è¡Œæ¸…ç©ºä¸Šæ¬¡çš„ debug ç»“æœ")
        print("="*60 + "\n")
    
    print(f"âš¡ å¹¶å‘é…ç½®: LLM x {args.llm_concurrency} | Image x {args.img_concurrency}")
    print(f"ğŸ”„ é‡è¯•é…ç½®: LLM {LLM_RETRY_TIMES}æ¬¡ | Image {IMG_RETRY_TIMES}æ¬¡")

    # 2. è¯»å–å†å²æ•°æ®ï¼ˆåŒºåˆ†å®Œæ•´å’Œä¸å®Œæ•´çš„æ•°æ®ï¼‰
    complete_data = []      # å®Œæ•´çš„æ•°æ®ï¼ˆæœ‰ cn_name å’Œ image_urlï¼‰
    incomplete_tags = set() # ä¸å®Œæ•´çš„ tagï¼ˆéœ€è¦é‡æ–°å¤„ç†ï¼‰
    existing_tags = set()   # æ‰€æœ‰å·²å­˜åœ¨çš„ tag
    
    # Debug æ¨¡å¼ï¼šå¿½ç•¥å†å²æ•°æ®ï¼Œå¤„ç†æ‰€æœ‰æŒ‡å®šçš„è§’è‰²
    if args.debug:
        print("ğŸ› Debug æ¨¡å¼ï¼šå¿½ç•¥å†å²æ•°æ®ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰è§’è‰²")
        # Debug æ¨¡å¼ä¸è¯»å–å†å²æ•°æ®
        complete_data = []
        incomplete_tags = set()
        existing_tags = set()
    elif os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            for item in history_data:
                tag = item.get('tag')
                cn_name = item.get('cn_name')
                image_url = item.get('image_url')
                
                if tag:
                    existing_tags.add(tag)
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦å®Œæ•´
                    is_complete = (
                        cn_name and str(cn_name).strip() and 
                        image_url and str(image_url).startswith('http')
                    )
                    
                    if is_complete:
                        complete_data.append(item)  # å®Œæ•´æ•°æ®ä¿ç•™
                    else:
                        incomplete_tags.add(tag)    # ä¸å®Œæ•´æ•°æ®æ ‡è®°ä¸ºå¾…å¤„ç†
        except Exception:
            complete_data = []
            incomplete_tags = set()
            existing_tags = set()

    # æ„å»ºå¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨ï¼ˆä¸¤éƒ¨åˆ†æ•°æ®ï¼‰
    # 1. è¿œç¨‹æ–°å¢çš„ tagï¼ˆæœ¬åœ°ä¸å­˜åœ¨ï¼‰
    # 2. æœ¬åœ°å·²æœ‰ä½†ä¸å®Œæ•´çš„ tagï¼ˆéœ€é‡æ–°å¤„ç†ï¼‰
    data_to_process = [
        {"tag": tag, "color": info["color"], "content": info["content"]}
        for tag, info in tags_dict.items()
        if tag not in existing_tags or tag in incomplete_tags
    ]

    if not data_to_process:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†ï¼")
        return

    print(f"ğŸ”¥ æœ¬æ¬¡éœ€å¤„ç†: {len(data_to_process)} ä¸ªè§’è‰²")

    # 3. åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    # ä½¿ç”¨åŒä¸€ä¸ª ClientSession å¯ä»¥å¤ç”¨ TCP è¿æ¥ï¼Œæ˜¾è‘—æå‡ SSL æ¡æ‰‹é€Ÿåº¦
    timeout = aiohttp.ClientTimeout(total=90) # ç»™æ•´ä¸ªé“¾è·¯æ›´é•¿çš„å®½å®¹åº¦
    async with aiohttp.ClientSession(timeout=timeout) as session:
        
        # å°†æ‰€æœ‰å¾…å¤„ç†æ•°æ®åˆ†ç»„
        batches = [data_to_process[i : i + args.batch_size] for i in range(0, len(data_to_process), args.batch_size)]
        
        tasks = []
        for batch in batches:
            # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡çš„åç¨‹ä»»åŠ¡
            # æ³¨æ„ï¼šå®ƒä»¬ä¸ä¼šç«‹åˆ»å…¨éƒ¨æ‰§è¡Œï¼Œè€Œæ˜¯ä¼šè¢«ä¿¡å·é‡(Semaphore)å¡ä½
            task = asyncio.create_task(pipeline_batch(session, batch))
            tasks.append(task)
        
        # 4. å¼‚æ­¥æ‰§è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
        # current_data ç”¨äºåœ¨å†…å­˜ä¸­ç´¯ç§¯æ•°æ®ï¼ˆåªä¿ç•™å®Œæ•´çš„æ—§æ•°æ®ï¼‰
        current_data = complete_data.copy()
        finished_batches = 0
        
        # ä½¿ç”¨è§’è‰²æ•°é‡è€Œä¸æ˜¯æ‰¹æ¬¡æ•°é‡æ¥æ˜¾ç¤ºè¿›åº¦
        total_characters = len(data_to_process)
        pbar = tqdm(total=total_characters, desc="ğŸš€ å¤„ç†ä¸­", unit="è§’è‰²")
        
        for coro in asyncio.as_completed(tasks):
            batch_result = await coro
            current_data.extend(batch_result)
            finished_batches += 1
            stats.total_processed += len(batch_result)
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆæŒ‰è§’è‰²æ•°é‡ï¼‰
            pbar.update(len(batch_result))
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºå®æ—¶æˆåŠŸç‡
            llm_total = stats.llm_success + stats.llm_fail
            img_total = stats.img_success + stats.img_fail
            postfix_dict = {}
            if llm_total > 0:
                postfix_dict['LLM'] = f"{stats.llm_success/llm_total*100:.0f}%"
            if img_total > 0:
                postfix_dict['å›¾ç‰‡'] = f"{stats.img_success/img_total*100:.0f}%"
            if postfix_dict:
                pbar.set_postfix(postfix_dict)
            
            # å®šæœŸå­˜ç›˜ï¼Œè€Œä¸æ˜¯æ¯æ‰¹æ¬¡éƒ½å­˜
            if finished_batches % SAVE_INTERVAL_BATCHES == 0:
                # Debug æ¨¡å¼è¾“å‡ºåˆ°ç‹¬ç«‹æ–‡ä»¶
                output_file = DEBUG_OUTPUT_FILE if args.debug else OUTPUT_FILE
                save_data(current_data, output_file)
        
        pbar.close()
        
        # æœ€åå†ä¸€æ¬¡æ€§ä¿å­˜ï¼Œç¡®ä¿æ•°æ®å®Œæ•´
        # Debug æ¨¡å¼è¾“å‡ºåˆ°ç‹¬ç«‹æ–‡ä»¶
        output_file = DEBUG_OUTPUT_FILE if args.debug else OUTPUT_FILE
        save_data(current_data, output_file)
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    stats.print_summary()
    
    if args.debug:
        print(f"\nğŸ› Debug æ¨¡å¼ï¼šæ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        print("âš ï¸  æ­£å¼æ–‡ä»¶æœªå—å½±å“")
    else:
        print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

if __name__ == '__main__':
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())