import asyncio
import json
import os
import aiohttp
import time
import random
import argparse
from typing import List, Dict
from tqdm.asyncio import tqdm
import os
import sys
from dotenv import load_dotenv

load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ================= é…ç½®åŒº =================
INPUT_URL = "https://raw.githubusercontent.com/DominikDoom/a1111-sd-webui-tagcomplete/refs/heads/main/tags/noob_characters-chants.json"
OUTPUT_FILE = os.path.join(BASE_DIR, '..', 'output', 'noob_characters-chants-en-cn.json')
MAPPING_FILE = os.path.join(BASE_DIR, 'source_name_mapping.json')

# --- LLM é…ç½® ---
LLM_API_URL = os.getenv("LLM_API_URL") 
LLM_API_KEY = os.getenv("LLM_API_KEY") 
LLM_MODEL = os.getenv("LLM_MODEL")


# æ‰¹å¤„ç†å¤§å° (ä¿æŒä¸å˜ï¼ŒDeepSeek ä¸€æ¬¡å¤„ç†å¤ªå¤šå®¹æ˜“å¹»è§‰)
BATCH_SIZE = 10

# --- å¹¶å‘æ§åˆ¶ ---
# LLM å¹¶å‘æ•°ï¼šåŒæ—¶å‘é€ç»™ DeepSeek çš„è¯·æ±‚æ•°
# å»ºè®® 3-5ï¼Œå¤ªé«˜å¯èƒ½ä¼šè§¦å‘ Rate Limit æˆ–è¶…æ—¶
LLM_CONCURRENCY = 5

# æœå›¾å¹¶å‘æ•°ï¼šå…¨å±€åŒæ—¶è¯·æ±‚ Safebooru çš„æ•°é‡
# Safebooru æ¯”è¾ƒå®½æ¾ï¼Œä½†åœ¨é«˜å¹¶å‘ä¸‹å»ºè®®è®¾ä¸º 10-20
IMG_CONCURRENCY = 10 

# å­˜ç›˜é¢‘ç‡ï¼šæ¯å¤„ç†å®Œå¤šå°‘ä¸ªæ‰¹æ¬¡å­˜ä¸€æ¬¡ç›˜ (å‡å°‘ IO å¼€é”€)
SAVE_INTERVAL_BATCHES = 5

# --- é‡è¯•é…ç½® ---
LLM_RETRY_TIMES = 3  # LLM é‡è¯•æ¬¡æ•°
LLM_RETRY_DELAY = 2  # LLM é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
IMG_RETRY_TIMES = 2  # å›¾ç‰‡æœç´¢é‡è¯•æ¬¡æ•°
IMG_RETRY_DELAY = 1  # å›¾ç‰‡æœç´¢é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

# --- Danbooru API é…ç½® ---
DANBOORU_API_BASE = "https://safebooru.donmai.us"
DANBOORU_RATE_LIMIT = 0.5  # 2 è¯·æ±‚/ç§’ (åŒ¿åç”¨æˆ·)
DANBOORU_RETRY_TIMES = 2  # Danbooru API é‡è¯•æ¬¡æ•°
DANBOORU_RETRY_DELAY = 1  # Danbooru API é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

# --- è°ƒè¯•æ¨¡å¼ï¼ˆé»˜è®¤å€¼ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼‰ ---
DEBUG_MODE = False
DEBUG_LIMIT = 10
DEBUG_RANDOM = True
# ===========================================

# å…¨å±€ä¿¡å·é‡ï¼ˆå°†åœ¨ main ä¸­æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆå§‹åŒ–ï¼‰
sem_llm = None
sem_img = None
sem_danbooru = None  # Danbooru API ä¿¡å·é‡

# å…¨å±€æ˜ å°„è¡¨ï¼ˆå°†åœ¨ main ä¸­åŠ è½½ï¼‰
source_name_mapping = None

# å…¨å±€ Danbooru ç¼“å­˜
copyright_cache = {}  # {tag: [copyright_tags]}
last_danbooru_request = 0  # ä¸Šæ¬¡è¯·æ±‚æ—¶é—´æˆ³

# å…¨å±€ç»Ÿè®¡å¯¹è±¡
class Stats:
    """æ€§èƒ½ç»Ÿè®¡ç±»"""
    def __init__(self):
        self.llm_success = 0
        self.llm_fail = 0
        self.img_success = 0
        self.img_fail = 0
        self.total_processed = 0
        self.start_time = time.time()
    
    def print_summary(self):
        duration = time.time() - self.start_time
        llm_total = self.llm_success + self.llm_fail
        img_total = self.img_success + self.img_fail
        
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        print("="*50)
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ¯ æ€»å¤„ç†: {self.total_processed} ä¸ªè§’è‰²")
        if duration > 0:
            print(f"âš¡ å¹³å‡é€Ÿåº¦: {self.total_processed / duration:.2f} ä¸ª/ç§’")
        print(f"\nğŸ¤– LLM ç¿»è¯‘:")
        if llm_total > 0:
            print(f"   âœ… æˆåŠŸ: {self.llm_success}/{llm_total} ({self.llm_success/llm_total*100:.1f}%)")
            print(f"   âŒ å¤±è´¥: {self.llm_fail}/{llm_total} ({self.llm_fail/llm_total*100:.1f}%)")
        print(f"\nğŸ–¼ï¸  å›¾ç‰‡æœç´¢:")
        if img_total > 0:
            print(f"   âœ… æˆåŠŸ: {self.img_success}/{img_total} ({self.img_success/img_total*100:.1f}%)")
            print(f"   âŒ å¤±è´¥: {self.img_fail}/{img_total} ({self.img_fail/img_total*100:.1f}%)")
        print("="*50)

stats = Stats()

def load_source_name_mapping(mapping_file: str) -> Dict:
    """
    åŠ è½½ä½œå“åç§°è§„èŒƒåŒ–æ˜ å°„è¡¨
    
    Returns:
        åŒ…å«è§„èŒƒåŒ–è§„åˆ™çš„å­—å…¸
    """
    try:
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        print(f"âœ… å·²åŠ è½½ä½œå“åç§°æ˜ å°„è¡¨: {mapping_file}")
        return mapping
    except FileNotFoundError:
        print(f"âš ï¸  è­¦å‘Š: æ˜ å°„è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file}")
        return None
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: åŠ è½½æ˜ å°„è¡¨å¤±è´¥: {e}")
        return None

def normalize_source_names(source_en: str, source_cn: str) -> tuple:
    """
    è§„èŒƒåŒ–ä½œå“è‹±æ–‡åå’Œä¸­æ–‡å
    
    Args:
        source_en: åŸå§‹è‹±æ–‡ä½œå“å
        source_cn: åŸå§‹ä¸­æ–‡ä½œå“å
    
    Returns:
        (è§„èŒƒåŒ–åçš„è‹±æ–‡å, è§„èŒƒåŒ–åçš„ä¸­æ–‡å)
    """
    if not source_name_mapping:
        return source_en, source_cn
    
    mappings = source_name_mapping.get('mappings', {})
    en_rules = mappings.get('english_normalization', {}).get('rules', {})
    cn_rules = mappings.get('chinese_normalization', {}).get('rules', {})
    standard_pairs = mappings.get('standard_pairs', {}).get('pairs', {})
    
    normalized_en = source_en
    normalized_cn = source_cn
    
    # 1. å…ˆå°è¯•è§„èŒƒåŒ–è‹±æ–‡å
    for standard_en, variants in en_rules.items():
        if source_en in variants:
            normalized_en = standard_en
            break
    
    # 2. å†å°è¯•è§„èŒƒåŒ–ä¸­æ–‡å
    for standard_cn, variants in cn_rules.items():
        if source_cn in variants:
            normalized_cn = standard_cn
            break
    
    # 3. å¦‚æœè‹±æ–‡åæ˜¯æ ‡å‡†çš„ï¼Œä¸”ä¸­æ–‡åä¸ºç©ºæˆ–ä¸æ ‡å‡†ï¼Œä½¿ç”¨æ ‡å‡†é…å¯¹
    if normalized_en in standard_pairs and (not normalized_cn or normalized_cn != standard_pairs[normalized_en]):
        normalized_cn = standard_pairs[normalized_en]
    
    # 4. å¦‚æœä¸­æ–‡åæ˜¯æ ‡å‡†çš„ï¼Œä¸”è‹±æ–‡åä¸ºç©ºæˆ–ä¸æ ‡å‡†ï¼Œåå‘æŸ¥æ‰¾æ ‡å‡†é…å¯¹
    if normalized_cn:
        for std_en, std_cn in standard_pairs.items():
            if normalized_cn == std_cn and (not normalized_en or normalized_en != std_en):
                normalized_en = std_en
                break
    
    return normalized_en, normalized_cn


async def fetch_copyright_from_danbooru(session: aiohttp.ClientSession, tag: str) -> List[str]:
    """
    ä» Danbooru API è·å–è§’è‰²çš„ç‰ˆæƒæ ‡ç­¾
    
    Args:
        tag: è§’è‰²æ ‡ç­¾å
    
    Returns:
        ç‰ˆæƒæ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["fate/grand_order", "fate_(series)"] æˆ–ç©ºåˆ—è¡¨
    """
    global last_danbooru_request
    
    # æ£€æŸ¥ç¼“å­˜
    if tag in copyright_cache:
        return copyright_cache[tag]
    
    # æ„å»º API è¯·æ±‚ URL
    # ä½¿ç”¨ tag search API æŸ¥è¯¢è§’è‰²æ ‡ç­¾çš„ç‰ˆæƒä¿¡æ¯
    api_url = f"{DANBOORU_API_BASE}/tags.json?search[name]={tag}"
    
    copyrights = []
    
    for attempt in range(DANBOORU_RETRY_TIMES):
        try:
            async with sem_danbooru:
                # é€Ÿç‡é™åˆ¶ï¼šç¡®ä¿ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´è‡³å°‘é—´éš” DANBOORU_RATE_LIMIT ç§’
                current_time = time.time()
                time_since_last = current_time - last_danbooru_request
                if time_since_last < DANBOORU_RATE_LIMIT:
                    await asyncio.sleep(DANBOORU_RATE_LIMIT - time_since_last)
                
                last_danbooru_request = time.time()
                
                async with session.get(api_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # å¦‚æœæ‰¾åˆ°æ ‡ç­¾ä¿¡æ¯
                        if data and isinstance(data, list) and len(data) > 0:
                            tag_info = data[0]
                            # æ£€æŸ¥æ˜¯å¦æœ‰å…³è”çš„ç‰ˆæƒæ ‡ç­¾
                            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢è§’è‰²å…³è”çš„å¸–å­æ¥è·å–ç‰ˆæƒä¿¡æ¯
                            # å› ä¸º tag API ä¸ç›´æ¥è¿”å›ç‰ˆæƒä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥è¯¢ä½¿ç”¨è¯¥æ ‡ç­¾çš„å¸–å­
                            
                            # æŸ¥è¯¢ä½¿ç”¨è¯¥æ ‡ç­¾çš„å¸–å­ï¼ˆé™åˆ¶1ä¸ªå³å¯ï¼‰
                            posts_url = f"{DANBOORU_API_BASE}/posts.json?tags={tag}&limit=1"
                            async with session.get(posts_url) as posts_response:
                                if posts_response.status == 200:
                                    posts_data = await posts_response.json()
                                    if posts_data and isinstance(posts_data, list) and len(posts_data) > 0:
                                        post = posts_data[0]
                                        # ä»å¸–å­çš„ tag_string_copyright å­—æ®µè·å–ç‰ˆæƒæ ‡ç­¾
                                        copyright_string = post.get('tag_string_copyright', '')
                                        if copyright_string:
                                            copyrights = [c.strip() for c in copyright_string.split() if c.strip()]
                        
                        # ç¼“å­˜ç»“æœ
                        copyright_cache[tag] = copyrights
                        return copyrights
                        
        except Exception as e:
            if attempt == DANBOORU_RETRY_TIMES - 1:
                # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                pass
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < DANBOORU_RETRY_TIMES - 1:
            await asyncio.sleep(DANBOORU_RETRY_DELAY)
    
    # ç¼“å­˜ç©ºç»“æœï¼Œé¿å…é‡å¤è¯·æ±‚
    copyright_cache[tag] = []
    return []


async def call_llm_custom(session: aiohttp.ClientSession, prompt: str) -> str:
    """è°ƒç”¨ LLM æ¥å£è·å–å…ƒæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": "You are a JSON generator helper."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1,
        "response_format": {"type": "json_object"}
    }

    # é‡è¯•é€»è¾‘
    for attempt in range(LLM_RETRY_TIMES):
        try:
            async with sem_llm: # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶ LLM å¹¶å‘
                async with session.post(LLM_API_URL, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        # ç»Ÿè®¡å·²ç§»è‡³ translate_batch_task æŒ‰è§’è‰²æ•°é‡ç»Ÿè®¡
                        return result['choices'][0]['message']['content']
                    else:
                        # æ‰“å°é”™è¯¯çŠ¶æ€ç ï¼Œæ–¹ä¾¿è°ƒè¯•
                        if attempt == LLM_RETRY_TIMES - 1:
                            print(f"\n[LLM Error] Status: {response.status} (å·²é‡è¯•{attempt+1}æ¬¡)")
        except Exception as e:
            if attempt == LLM_RETRY_TIMES - 1:
                print(f"\n[LLM] è¯·æ±‚å¼‚å¸¸: {e} (å·²é‡è¯•{attempt+1}æ¬¡)")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < LLM_RETRY_TIMES - 1:
            await asyncio.sleep(LLM_RETRY_DELAY * (attempt + 1))  # æŒ‡æ•°é€€é¿
    
    # ç»Ÿè®¡å¤±è´¥å·²ç§»è‡³ translate_batch_task
    return None

async def search_image_safebooru(session: aiohttp.ClientSession, tag: str) -> str:
    """Safebooru æœå›¾ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    url = f"https://safebooru.org/index.php?page=dapi&s=post&q=index&tags={tag}+solo&limit=1&json=1"
    
    # é‡è¯•é€»è¾‘
    for attempt in range(IMG_RETRY_TIMES):
        try:
            # ä½¿ç”¨å…¨å±€ä¿¡å·é‡é™åˆ¶å›¾ç‰‡å¹¶å‘
            async with sem_img:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        data = await resp.json(content_type=None)
                        if data and isinstance(data, list) and len(data) > 0:
                            img = data[0]
                            stats.img_success += 1
                            return f"https://safebooru.org/images/{img['directory']}/{img['image']}"
                # ç»™æœåŠ¡å™¨å–˜æ¯
                await asyncio.sleep(0.2)
        except Exception:
            pass
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if attempt < IMG_RETRY_TIMES - 1:
            await asyncio.sleep(IMG_RETRY_DELAY)
    
    stats.img_fail += 1
    return None

async def process_images_for_list(session: aiohttp.ClientSession, data_list: List[Dict]) -> List[Dict]:
    """ä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¡ç›®å¹¶å‘è·å–å›¾ç‰‡"""
    tasks = []
    
    async def _task(item):
        # å¦‚æœå·²ç»æœ‰å›¾ï¼Œç›´æ¥è¿”å›
        if item.get('image_url') and str(item['image_url']).startswith('http'):
            return item
        
        # æ²¡å›¾åˆ™å»æœ
        img_url = await search_image_safebooru(session, item['tag'])
        item['image_url'] = img_url
        
        # ç®€å•çš„æ—¥å¿—è¾“å‡º (å¯é€‰ï¼Œé«˜å¹¶å‘ä¸‹åˆ·å±å¯æ³¨é‡Šæ‰)
        # log_name = item.get('cn_name') or item['tag']
        # status = "âœ…" if img_url else "âŒ"
        # print(f"{status} {log_name}", end="\r") 
        
        return item

    for item in data_list:
        tasks.append(_task(item))
    
    # ç­‰å¾…è¯¥æ‰¹æ¬¡æ‰€æœ‰å›¾ç‰‡ä»»åŠ¡å®Œæˆ
    return await asyncio.gather(*tasks)

async def translate_batch_task(session: aiohttp.ClientSession, batch_data: List[Dict]) -> List[Dict]:
    """
    LLM ç¿»è¯‘ä»»åŠ¡
    
    Args:
        batch_data: åŒ…å« {"tag": str, "color": int, "content": str, "copyrights": List[str]} çš„åˆ—è¡¨
    """
    # æ„å»ºå¸¦ç‰ˆæƒä¿¡æ¯çš„æ ‡ç­¾åˆ—è¡¨
    tags_with_copyright = [
        {
            "tag": item['tag'],
            "copyrights": item.get('copyrights', [])
        }
        for item in batch_data
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•éç©ºçš„ç‰ˆæƒä¿¡æ¯
    has_copyright_info = any(item['copyrights'] for item in tags_with_copyright)
    
    # æ ¹æ®æ˜¯å¦æœ‰ç‰ˆæƒä¿¡æ¯ï¼ŒåŠ¨æ€è°ƒæ•´ prompt
    if has_copyright_info:
        copyright_instruction = """æ¯ä¸ªè§’è‰²éƒ½é™„å¸¦äº†ä» Danbooru è·å–çš„ç‰ˆæƒæ ‡ç­¾ï¼ˆcopyrightsï¼‰ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥ç¡®å®šä½œå“åç§°ã€‚

æ ‡ç­¾åˆ—è¡¨ï¼ˆå«ç‰ˆæƒä¿¡æ¯ï¼‰:"""
        
        source_rules = """3. source_cn å’Œ source_en å¡«å†™è§„åˆ™ï¼ˆé‡è¦ï¼‰ï¼š
   - **ä¼˜å…ˆä½¿ç”¨æä¾›çš„ copyrights ä¿¡æ¯**æ¥ç¡®å®šä½œå“åç§°
   - å¦‚æœ copyrights åŒ…å«ä½œå“æ ‡ç­¾ï¼ˆå¦‚ ["date_a_live"]ï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºè§„èŒƒçš„ä½œå“åç§°
   - ä½œå“åç§°è½¬æ¢è§„åˆ™ï¼š
     * "date_a_live" â†’ source_en: "Date A Live", source_cn: "çº¦ä¼šå¤§ä½œæˆ˜"
     * "fate/grand_order" æˆ– "fate_(series)" â†’ source_en: "Fate/Grand Order", source_cn: "å‘½è¿/å† ä½æŒ‡å®š"
     * "blue_archive" â†’ source_en: "Blue Archive", source_cn: "è”šè“æ¡£æ¡ˆ"
     * "original" â†’ source_en: "Original", source_cn: "" (åŸåˆ›è§’è‰²/VTuber)
   - å¦‚æœ copyrights ä¸ºç©ºæˆ–åŒ…å« "original"ï¼Œä¸”ä½ èƒ½ä»è§’è‰²åæ¨æ–­å‡ºå‡†ç¡®æ¥æºï¼Œå¯ä»¥ä½¿ç”¨æ¨æ–­ç»“æœ
   - å¦‚æœå®Œå…¨æ— æ³•ç¡®å®šï¼Œsource_en å¡«å†™ "Original"ï¼Œsource_cn ç•™ç©º"""
    else:
        copyright_instruction = """æ ‡ç­¾åˆ—è¡¨:"""
        
        source_rules = """3. source_cn å’Œ source_en å¡«å†™è§„åˆ™ï¼ˆé‡è¦ï¼‰ï¼š
   - **æ ¹æ®è§’è‰²æ ‡ç­¾åç§°æ¨æ–­ä½œå“æ¥æº**ï¼ˆæ²¡æœ‰æä¾›ç‰ˆæƒä¿¡æ¯ï¼‰
   - å¦‚æœè§’è‰²åä¸­åŒ…å«ä½œå“æç¤ºï¼ˆå¦‚æ‹¬å·ä¸­çš„ç³»åˆ—åï¼‰ï¼Œä½¿ç”¨è¯¥ä¿¡æ¯
   - ä½œå“åç§°è½¬æ¢è§„åˆ™ç¤ºä¾‹ï¼š
     * åŒ…å« "_(fate)" â†’ é€šå¸¸æ˜¯ Fate ç³»åˆ—ä½œå“
     * åŒ…å« "_(blue_archive)" â†’ Blue Archive / è”šè“æ¡£æ¡ˆ
     * åŒ…å« "_(kancolle)" â†’ Kantai Collection / èˆ°é˜ŸCollection
   - å¦‚æœè§’è‰²åä¸­æ²¡æœ‰ä½œå“æç¤ºï¼Œæ ¹æ®ä½ çš„ ACG çŸ¥è¯†æ¨æ–­
   - å¦‚æœå®Œå…¨æ— æ³•ç¡®å®šæˆ–æ˜¯åŸåˆ›è§’è‰²/VTuberï¼Œsource_en å¡«å†™ "Original"ï¼Œsource_cn ç•™ç©º"""
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç²¾é€šACGæ–‡åŒ–çš„ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ Danbooru Character Tags ç¿»è¯‘æˆ JSON æ ¼å¼ã€‚

{copyright_instruction}
{json.dumps(tags_with_copyright, ensure_ascii=False)}

ç¿»è¯‘è¦æ±‚:
1. è¿”å›çº¯ JSON æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
   - "tag": åŸæ ‡ç­¾ï¼ˆä¿æŒä¸å˜ï¼‰
   - "cn_name": ä¸­æ–‡è§’è‰²å
   - "cn_name_status": ä¸­æ–‡åçŠ¶æ€æ ‡æ³¨
   - "en_name": è‹±æ–‡è§’è‰²åï¼ˆå»æ‰ä¸‹åˆ’çº¿ï¼Œé¦–å­—æ¯å¤§å†™ï¼‰
   - "source_cn": ä½œå“ä¸­æ–‡å
   - "source_en": ä½œå“è‹±æ–‡å

2. cn_name å’Œ cn_name_status å¡«å†™è§„åˆ™ï¼ˆé‡è¦ï¼‰ï¼š
   - å¦‚æœæ˜¯çŸ¥åè§’è‰²ï¼Œå¡«å†™å®˜æ–¹ä¸­æ–‡è¯‘åï¼Œcn_name_status å¡«å†™ true
   - å¦‚æœæ˜¯æ—¥æ–‡è§’è‰²ä½†æ²¡æœ‰å®˜æ–¹ä¸­æ–‡åï¼Œè¿›è¡Œåˆç†éŸ³è¯‘ï¼ˆå¦‚ï¼šãƒªãƒ³ â†’ å‡›ï¼Œè¾“å‡º`ãƒªãƒ³ï¼ˆå‡›ï¼‰`ï¼‰ï¼Œcn_name_status å¡«å†™ "éŸ³è¯‘"
   - å¦‚æœæ˜¯è‹±æ–‡è§’è‰²åï¼Œå¯ä»¥éŸ³è¯‘æˆ–ç•™ç©ºï¼ˆå¦‚ï¼šAsia Argento å¯ä»¥éŸ³è¯‘ä¸º"äºšç»†äºšÂ·é˜¿å°”çœŸæ‰˜"ï¼‰ï¼Œcn_name_status å¡«å†™ "éŸ³è¯‘"
   - å¦‚æœå®Œå…¨ä¸çŸ¥é“æˆ–æ— æ³•æ¨æ–­ï¼Œcn_name ç•™ç©ºï¼Œcn_name_status å¡«å†™ "æœªçŸ¥"

{source_rules}

4. ä¸¥ç¦ä½¿ç”¨ Markdown ä»£ç å—åŒ…è£¹ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„ã€‚

ç¤ºä¾‹ï¼š
[
  {{"tag": "hatsune_miku", "cn_name": "åˆéŸ³æœªæ¥", "cn_name_status": "", "en_name": "Hatsune Miku", "source_cn": "Vocaloid", "source_en": "Vocaloid"}},
  {{"tag": "yamai_yuzuru", "cn_name": "å…«èˆå¤•å¼¦", "cn_name_status": "", "en_name": "Yamai Yuzuru", "source_cn": "çº¦ä¼šå¤§ä½œæˆ˜", "source_en": "Date A Live"}},
  {{"tag": "rin_(vocaloid)", "cn_name": "ãƒªãƒ³ï¼ˆå‡›ï¼‰", "cn_name_status": "éŸ³è¯‘", "en_name": "Rin", "source_cn": "Vocaloid", "source_en": "Vocaloid"}},
  {{"tag": "unknown_character_xyz", "cn_name": "", "cn_name_status": "æœªçŸ¥", "en_name": "Unknown Character Xyz", "source_cn": "", "source_en": "Original"}}
]
"""
    
    content = await call_llm_custom(session, prompt)
    
    # æ„é€ é»˜è®¤è¿”å›å€¼ï¼Œé˜²æ­¢ LLM æŒ‚äº†å¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡ä¸¢å¤±
    # åŒæ—¶ä¿ç•™åŸå§‹çš„ color å’Œ content å­—æ®µ
    # LLMé”™è¯¯æ—¶ cn_name_status ç•™ç©ºï¼Œæ–¹ä¾¿åŒºåˆ†çœŸæ­£çš„"æœªçŸ¥"çŠ¶æ€
    default_res = [
        {
            "tag": item['tag'], 
            "cn_name": "", 
            "cn_name_status": "",  # LLMé”™è¯¯æ—¶ç•™ç©º
            "en_name": item['tag'], 
            "source_cn": "", 
            "source_en": "",
            "color": item['color'],
            "content": item['content']
        } 
        for item in batch_data
    ]

    if not content:
        print("\nâš ï¸ LLM è¿”å›å†…å®¹ä¸ºç©º")
        # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
        stats.llm_fail += len(batch_data)
        return default_res

    try:
        print(f"\n[DEBUG] LLMåŸå§‹è¿”å›å‰100å­—ç¬¦: {content[:100]}")
        
        clean_content = content.replace("```json", "").replace("```", "").strip()
        result = json.loads(clean_content)
        
        print(f"[DEBUG] è§£æåç±»å‹: {type(result)}")
        
        # å…¼å®¹ LLM å¯èƒ½è¿”å› {"items": [...]} æˆ–ç›´æ¥ [...] çš„æƒ…å†µ
        items = None
        if isinstance(result, dict):
            print(f"[DEBUG] è¿”å›çš„æ˜¯å­—å…¸ï¼Œé”®: {list(result.keys())}")
            for val in result.values():
                if isinstance(val, list): 
                    items = val
                    print(f"[DEBUG] ä»å­—å…¸ä¸­æå–åˆ°åˆ—è¡¨ï¼Œé•¿åº¦: {len(items)}")
                    break
        elif isinstance(result, list):
            items = result
            print(f"[DEBUG] è¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(items)}")
        
        if not items:
            print("\nâš ï¸ æ— æ³•ä» LLM è¿”å›ä¸­æå–åˆ—è¡¨æ•°æ®")
            print(f"[DEBUG] è¿”å›å†…å®¹: {json.dumps(result, ensure_ascii=False)[:200]}")
            # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
            stats.llm_fail += len(batch_data)
            return default_res
        
        print(f"âœ… è§£ææˆåŠŸï¼Œå…± {len(items)} é¡¹")
        # è°ƒè¯•ï¼šè¾“å‡ºè§£æåçš„ç¬¬ä¸€ä¸ªé¡¹ç›®
        if items:
            print(f"âœ… è§£ææˆåŠŸï¼Œå…± {len(items)} é¡¹")
        
        # å°† color å’Œ content å­—æ®µåˆå¹¶åˆ° LLM è¿”å›çš„ç»“æœä¸­
        tag_to_data = {item['tag']: item for item in batch_data}
        for item in items:
            tag = item.get('tag')
            if tag and tag in tag_to_data:
                item['color'] = tag_to_data[tag]['color']
                item['content'] = tag_to_data[tag]['content']
            
            # è§„èŒƒåŒ–ä½œå“åç§°
            source_en = item.get('source_en', '')
            source_cn = item.get('source_cn', '')
            normalized_en, normalized_cn = normalize_source_names(source_en, source_cn)
            item['source_en'] = normalized_en
            item['source_cn'] = normalized_cn
        
        # ç»Ÿè®¡æˆåŠŸçš„è§’è‰²æ•°é‡
        stats.llm_success += len(items)
        return items
    except Exception as e:
        print(f"\nâŒ LLM æ•°æ®è§£æå¼‚å¸¸: {e}")
        # ç»Ÿè®¡å¤±è´¥çš„è§’è‰²æ•°é‡
        stats.llm_fail += len(batch_data)
        return default_res

async def pipeline_batch(session: aiohttp.ClientSession, batch_data: List[Dict]) -> List[Dict]:
    """
    å•ä¸ªæ‰¹æ¬¡çš„å®Œæ•´æµæ°´çº¿ï¼š
    1. ç­‰å¾… LLM ä¿¡å·é‡ -> è¯·æ±‚ LLM
    2. è·å–åˆ° JSON -> è¯·æ±‚ Images (å†…éƒ¨æœ‰ Image ä¿¡å·é‡)
    3. è¿”å›ç»“æœ
    
    Args:
        batch_data: åŒ…å« {"tag": str, "color": int, "content": str} çš„åˆ—è¡¨
    """
    # 1. LLM é˜¶æ®µ
    translated_items = await translate_batch_task(session, batch_data)
    
    # 2. æœå›¾é˜¶æ®µ
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦å†æ˜¾å¼åŠ é”ï¼Œå› ä¸º search_image_safebooru å†…éƒ¨æœ‰ sem_img æ§åˆ¶
    final_items = await process_images_for_list(session, translated_items)
    
    return final_items

def save_data(data: List[Dict]):
    """è¾…åŠ©å‡½æ•°ï¼šä¿å­˜æ•°æ®åˆ°ç£ç›˜"""
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")

# LLM é…ç½®å®Œæ•´æ€§æ£€æµ‹
def check_llm_config():
    # æ£€æŸ¥ URL æ˜¯å¦ä¸ºç©º
    if not LLM_API_URL or not LLM_API_URL.strip():
        print("\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ LLM_API_URL æœªé…ç½®ï¼")
        print("ğŸ’¡ æç¤ºï¼šè¯·è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œæˆ–ä½¿ç”¨ .env æ–‡ä»¶ã€‚")
        sys.exit(1)

    # æ£€æŸ¥ Key æ˜¯å¦ä¸ºç©º
    if not LLM_API_KEY or not LLM_API_KEY.strip():
        print("\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ LLM_API_KEY æœªé…ç½®ï¼")
        print("ğŸ’¡ æç¤ºï¼šè¯·è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ LLM_API_KEYã€‚")
        sys.exit(1)

async def fetch_tags_from_url(url: str) -> Dict[str, Dict]:
    """
    ä»æŒ‡å®š URL è·å–è§’è‰²æ ‡ç­¾æ•°æ®ï¼Œå¹¶ä» Danbooru è·å–ç‰ˆæƒä¿¡æ¯
    
    Args:
        url: JSON æ•°æ®çš„ URL åœ°å€
    
    Returns:
        å­—å…¸ï¼Œkey ä¸ºè§’è‰² nameï¼Œvalue ä¸ºåŒ…å« colorã€content å’Œ copyrights çš„å­—å…¸
        æ ¼å¼: {"character_name": {"color": 4, "content": "...", "copyrights": [...]}}
        å¦‚æœè·å–å¤±è´¥è¿”å›ç©ºå­—å…¸
    
    Note:
        æ•°æ®æ ¼å¼ç¤ºä¾‹:
        [
            {
                "name": "00_gundam",
                "terms": "Character",
                "content": "00 gundam,no humans, blue eyes...",
                "color": 4
            }
        ]
        åªæå– terms == "Character" çš„æ¡ç›®
    """
    print(f"ğŸ“¥ æ­£åœ¨ä» URL è·å–æ•°æ®: {url}")
    timeout = aiohttp.ClientTimeout(total=30)
    
    try:
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                if response.status != 200:
                    print(f"âŒ é”™è¯¯: æ— æ³•è·å–æ•°æ®ï¼ŒçŠ¶æ€ç : {response.status}")
                    return {}
                
                # GitHub raw æ–‡ä»¶è¿”å› text/plainï¼Œéœ€è¦å¿½ç•¥ Content-Type æ£€æŸ¥
                data = await response.json(content_type=None)
                
                # ä» JSON æ•°ç»„ä¸­æå– terms ä¸º "Character" çš„æ•°æ®
                if isinstance(data, list):
                    # æ„å»ºå­—å…¸: {name: {color, content}}
                    tags_dict = {}
                    for item in data:
                        if item.get('name') and item.get('terms') == 'Character':
                            tags_dict[item['name']] = {
                                'color': item.get('color', 0),
                                'content': item.get('content', ''),
                                'copyrights': []  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                            }
                    print(f"âœ… æˆåŠŸè·å– {len(tags_dict)} ä¸ªè§’è‰²æ ‡ç­¾")
                    return tags_dict
                elif isinstance(data, dict):
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè¿”å›ç©ºï¼ˆä¸æ”¯æŒï¼‰
                    print(f"âš ï¸ è­¦å‘Š: æ•°æ®ä¸ºå­—å…¸æ ¼å¼ï¼Œè¯·æ£€æŸ¥ URL")
                    return {}
                else:
                    print(f"âŒ é”™è¯¯: æœªçŸ¥çš„æ•°æ®æ ¼å¼")
                    return {}
                
    except Exception as e:
        print(f"âŒ é”™è¯¯: è·å–æ•°æ®å¤±è´¥ - {e}")
        return {}

def apply_debug_filter(tags_dict: Dict[str, Dict], debug_mode: bool, debug_limit: int, debug_random: bool) -> Dict[str, Dict]:
    """
    åº”ç”¨æ•°é‡é™åˆ¶è¿‡æ»¤
    
    Args:
        tags_dict: å®Œæ•´çš„æ ‡ç­¾å­—å…¸
        debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆå¯ç”¨éšæœºæŠ½å–ï¼‰
        debug_limit: æ•°é‡é™åˆ¶ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        debug_random: æ˜¯å¦éšæœºæŠ½å–
    
    Returns:
        è¿‡æ»¤åçš„æ ‡ç­¾å­—å…¸
    """
    # å¦‚æœæœªè®¾ç½®é™åˆ¶ï¼Œè¿”å›å…¨éƒ¨
    if debug_limit == 0:
        return tags_dict
    
    original_count = len(tags_dict)
    all_tags = list(tags_dict.keys())
    
    if debug_random:
        # éšæœºæŠ½å–
        selected_tags = random.sample(all_tags, min(debug_limit, len(all_tags)))
        print(f"ğŸ” é™é‡æ¨¡å¼: éšæœºæŠ½å– {len(selected_tags)}/{original_count} æ¡æ•°æ®")
    else:
        # æŒ‰é¡ºåºå–å‰Næ¡
        selected_tags = all_tags[:debug_limit]
        print(f"ğŸ” é™é‡æ¨¡å¼: å–å‰ {len(selected_tags)}/{original_count} æ¡æ•°æ®")
    
    # è¿”å›è¿‡æ»¤åçš„å­—å…¸
    return {tag: tags_dict[tag] for tag in selected_tags}

async def enrich_with_copyrights(session: aiohttp.ClientSession, data_list: List[Dict]) -> List[Dict]:
    """
    ä¸ºå¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨æ‰¹é‡è·å–ç‰ˆæƒä¿¡æ¯
    
    Args:
        session: aiohttp ClientSession
        data_list: åŒ…å« {"tag": str, ...} çš„åˆ—è¡¨
    
    Returns:
        æ·»åŠ äº† "copyrights" å­—æ®µçš„æ•°æ®åˆ—è¡¨
    """
    if not data_list:
        return data_list
    
    print(f"ğŸ” æ­£åœ¨ä» Danbooru è·å– {len(data_list)} ä¸ªè§’è‰²çš„ç‰ˆæƒä¿¡æ¯...")
    
    from tqdm import tqdm
    pbar = tqdm(total=len(data_list), desc="è·å–ç‰ˆæƒ", unit="è§’è‰²")
    
    for item in data_list:
        tag = item['tag']
        copyrights = await fetch_copyright_from_danbooru(session, tag)
        item['copyrights'] = copyrights
        pbar.update(1)
    
    pbar.close()
    print(f"âœ… ç‰ˆæƒä¿¡æ¯è·å–å®Œæˆ")
    return data_list

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¼‚æ­¥æ‰¹é‡å¤„ç†åŠ¨æ¼«è§’è‰²æ•°æ®ï¼šLLMç¿»è¯‘ + å›¾ç‰‡æœç´¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  # åªå¤„ç†å‰10000ä¸ªè§’è‰²ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡ç‰ˆæƒè·å–ï¼‰
  python %(prog)s --limit 10000 --skip-copyright
  
  # å¤„ç†10000ä¸ªè§’è‰²ï¼ˆåŒ…å«ç‰ˆæƒä¿¡æ¯ï¼Œè¾ƒæ…¢ï¼‰
  python %(prog)s --limit 10000
  
  # ç”Ÿäº§æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ•°æ®
  python %(prog)s
  
  # è‡ªå®šä¹‰å¹¶å‘æ•°
  python %(prog)s --llm-concurrency 10 --img-concurrency 20
        ''')
    
    # æ•°æ®å¤„ç†é€‰é¡¹
    parser.add_argument('--debug', action='store_true', 
                        help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆéšæœºæŠ½å–æ•°æ®ï¼‰')
    parser.add_argument('--limit', type=int, default=0, 
                        help='é™åˆ¶å¤„ç†çš„æ•°æ®é‡ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œé»˜è®¤: 0ï¼‰ã€‚ä¾‹å¦‚ --limit 10000 åªå¤„ç†10000ä¸ªè§’è‰²')
    parser.add_argument('--random', action='store_true', 
                        help='éšæœºæŠ½å–æ•°æ®ï¼ˆé»˜è®¤ï¼šæŒ‰é¡ºåºï¼‰ã€‚éœ€é…åˆ --limit ä½¿ç”¨')
    parser.add_argument('--skip-copyright', action='store_true',
                        help='è·³è¿‡ Danbooru ç‰ˆæƒä¿¡æ¯è·å–ï¼ˆæ˜¾è‘—æå‡é€Ÿåº¦ï¼Œä½†ä¾èµ– LLM æ¨æ–­ä½œå“æ¥æºï¼‰')
    
    # å¹¶å‘æ§åˆ¶
    parser.add_argument('--llm-concurrency', type=int, default=LLM_CONCURRENCY,
                        help=f'LLM å¹¶å‘æ•°ï¼ˆé»˜è®¤: {LLM_CONCURRENCY}ï¼‰')
    parser.add_argument('--img-concurrency', type=int, default=IMG_CONCURRENCY,
                        help=f'å›¾ç‰‡æœç´¢å¹¶å‘æ•°ï¼ˆé»˜è®¤: {IMG_CONCURRENCY}ï¼‰')
    
    # æ‰¹å¤„ç†é…ç½®
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                        help=f'æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤: {BATCH_SIZE}ï¼‰')
    
    return parser.parse_args()

async def main():
    global sem_llm, sem_img, sem_danbooru, source_name_mapping
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # åŠ è½½ä½œå“åç§°æ˜ å°„è¡¨
    source_name_mapping = load_source_name_mapping(MAPPING_FILE)
    
    # åˆå§‹åŒ–ä¿¡å·é‡
    sem_llm = asyncio.Semaphore(args.llm_concurrency)
    sem_img = asyncio.Semaphore(args.img_concurrency)
    sem_danbooru = asyncio.Semaphore(5)  # Danbooru API å¹¶å‘é™åˆ¶ä¸º5
    
    check_llm_config()

    # 1. ä» URL è¯»å–è¾“å…¥æ•°æ®
    tags_dict = await fetch_tags_from_url(INPUT_URL)
    
    if not tags_dict:
        print("âŒ é”™è¯¯: æ— æ³•è·å–æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®")
        return
    
    print(f"ğŸš€ è¾“å…¥æ€»æ•°: {len(tags_dict)}")
    
    # åº”ç”¨è°ƒè¯•æ¨¡å¼è¿‡æ»¤
    tags_dict = apply_debug_filter(tags_dict, args.debug, args.limit, args.random)
    
    print(f"âš¡ å¹¶å‘é…ç½®: LLM x {args.llm_concurrency} | Image x {args.img_concurrency}")
    print(f"ğŸ”„ é‡è¯•é…ç½®: LLM {LLM_RETRY_TIMES}æ¬¡ | Image {IMG_RETRY_TIMES}æ¬¡")

    # 2. è¯»å–å†å²æ•°æ®ï¼ˆåŒºåˆ†å®Œæ•´å’Œä¸å®Œæ•´çš„æ•°æ®ï¼‰
    complete_data = []      # å®Œæ•´çš„æ•°æ®ï¼ˆæœ‰ cn_name å’Œ image_urlï¼‰
    incomplete_tags = set() # ä¸å®Œæ•´çš„ tagï¼ˆéœ€è¦é‡æ–°å¤„ç†ï¼‰
    existing_tags = set()   # æ‰€æœ‰å·²å­˜åœ¨çš„ tag
    
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            for item in history_data:
                tag = item.get('tag')
                cn_name = item.get('cn_name')
                image_url = item.get('image_url')
                
                if tag:
                    existing_tags.add(tag)
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦å®Œæ•´
                    is_complete = (
                        cn_name and str(cn_name).strip() and 
                        image_url and str(image_url).startswith('http')
                    )
                    
                    if is_complete:
                        complete_data.append(item)  # å®Œæ•´æ•°æ®ä¿ç•™
                    else:
                        incomplete_tags.add(tag)    # ä¸å®Œæ•´æ•°æ®æ ‡è®°ä¸ºå¾…å¤„ç†
        except Exception:
            complete_data = []
            incomplete_tags = set()
            existing_tags = set()

    # æ„å»ºå¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨ï¼ˆä¸¤éƒ¨åˆ†æ•°æ®ï¼‰
    # 1. è¿œç¨‹æ–°å¢çš„ tagï¼ˆæœ¬åœ°ä¸å­˜åœ¨ï¼‰
    # 2. æœ¬åœ°å·²æœ‰ä½†ä¸å®Œæ•´çš„ tagï¼ˆéœ€é‡æ–°å¤„ç†ï¼‰
    data_to_process = [
        {"tag": tag, "color": info["color"], "content": info["content"]}
        for tag, info in tags_dict.items()
        if tag not in existing_tags or tag in incomplete_tags
    ]

    if not data_to_process:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†ï¼")
        return

    print(f"ğŸ”¥ æœ¬æ¬¡éœ€å¤„ç†: {len(data_to_process)} ä¸ªè§’è‰²")
    
    # ä¸ºå¾…å¤„ç†çš„è§’è‰²è·å–ç‰ˆæƒä¿¡æ¯ï¼ˆå¦‚æœæœªè·³è¿‡ï¼‰
    if not args.skip_copyright:
        timeout = aiohttp.ClientTimeout(total=90)
        async with aiohttp.ClientSession(timeout=timeout) as copyright_session:
            data_to_process = await enrich_with_copyrights(copyright_session, data_to_process)
    else:
        print("âš¡ å·²è·³è¿‡ Danbooru ç‰ˆæƒä¿¡æ¯è·å–ï¼ˆä½¿ç”¨ --skip-copyrightï¼‰")
        # ä¸ºæ¯ä¸ªè§’è‰²æ·»åŠ ç©ºçš„ copyrights å­—æ®µ
        for item in data_to_process:
            item['copyrights'] = []

    # 3. åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    # ä½¿ç”¨åŒä¸€ä¸ª ClientSession å¯ä»¥å¤ç”¨ TCP è¿æ¥ï¼Œæ˜¾è‘—æå‡ SSL æ¡æ‰‹é€Ÿåº¦
    timeout = aiohttp.ClientTimeout(total=90) # ç»™æ•´ä¸ªé“¾è·¯æ›´é•¿çš„å®½å®¹åº¦
    async with aiohttp.ClientSession(timeout=timeout) as session:
        
        # å°†æ‰€æœ‰å¾…å¤„ç†æ•°æ®åˆ†ç»„
        batches = [data_to_process[i : i + args.batch_size] for i in range(0, len(data_to_process), args.batch_size)]
        
        tasks = []
        for batch in batches:
            # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡çš„åç¨‹ä»»åŠ¡
            # æ³¨æ„ï¼šå®ƒä»¬ä¸ä¼šç«‹åˆ»å…¨éƒ¨æ‰§è¡Œï¼Œè€Œæ˜¯ä¼šè¢«ä¿¡å·é‡(Semaphore)å¡ä½
            task = asyncio.create_task(pipeline_batch(session, batch))
            tasks.append(task)
        
        # 4. å¼‚æ­¥æ‰§è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
        # current_data ç”¨äºåœ¨å†…å­˜ä¸­ç´¯ç§¯æ•°æ®ï¼ˆåªä¿ç•™å®Œæ•´çš„æ—§æ•°æ®ï¼‰
        current_data = complete_data.copy()
        finished_batches = 0
        
        # ä½¿ç”¨è§’è‰²æ•°é‡è€Œä¸æ˜¯æ‰¹æ¬¡æ•°é‡æ¥æ˜¾ç¤ºè¿›åº¦
        total_characters = len(data_to_process)
        pbar = tqdm(total=total_characters, desc="ğŸš€ å¤„ç†ä¸­", unit="è§’è‰²")
        
        for coro in asyncio.as_completed(tasks):
            batch_result = await coro
            current_data.extend(batch_result)
            finished_batches += 1
            stats.total_processed += len(batch_result)
            
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆæŒ‰è§’è‰²æ•°é‡ï¼‰
            pbar.update(len(batch_result))
            
            # å®šæœŸå­˜ç›˜ï¼Œè€Œä¸æ˜¯æ¯æ‰¹æ¬¡éƒ½å­˜
            if finished_batches % SAVE_INTERVAL_BATCHES == 0:
                save_data(current_data)
                pbar.set_postfix({"å·²ä¿å­˜": len(current_data)})
        
        pbar.close()
        
        # æœ€åå†ä¸€æ¬¡æ€§ä¿å­˜ï¼Œç¡®ä¿æ•°æ®å®Œæ•´
        save_data(current_data)
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    stats.print_summary()
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ {OUTPUT_FILE}")

if __name__ == '__main__':
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())